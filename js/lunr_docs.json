

[


    { 
        "title" : "A sketchpad project based on Sinatra, Nginx using docker-compose",

        
        
        
        "tags": [
        
            "sinatra",
            
        
            "nginx",
            
        
            "docker-compose"
            
        
        ],
        "href" : "2018/sketchpad-project-sinatra-nginx-docker-compose",
        "content" : "I recently came across a monolithic application at work that used various frameworks, which in turn made extending existing routes very difficult to implement.The general consensus amongst my peers was to stick a proxy in front of the application. The only decision left was to determine if we could do this using the existing reverse proxy (Nginx), or write a bespoke proxy (between Nginx and the existing monolith).The requirement for the proxy was to take a value in the URL, and pass it on as a request parameter to the monolith.An example incoming URL might look like this /api/parp/foo. We want this URL to be rewritten to the monolith as /api/foo and parp to be appended to the request parameter as value for the key fart_noise. The only liberty I've taken with the solution is that the value of fart_noise will always be parp in this scenario.You can do this easily enough in Nginx using proxy_set_body directive.In addition to this requirement, I needed to leave incoming URLs without parp as-is, this is because we expect the client to provide an alternative value for fart_noise i.e. toot.I created a test server to simulate the monolith using Sinatra. Which is great for creating REST based APIs, I think it actually edges Flask out for simplicity.I used the namespace directive to add prefix /api to the existing end points i.e. /api/foo. I only needed to prove the URL rewrites would work for GET and POST methods, so I echoed the params variable back as a response which allowed me to verify the parameters were being modified correctly.class MyWay &lt; Sinatra::Base  register Sinatra::Namespace  namespace &#39;/api&#39; do    get &#39;/foo&#39; do      logger.info &quot;api/foo parameters: #{params}&quot;      params.inspect    end    post &#39;/bar&#39; do      logger.info &quot;api/bar parameters: #{params}&quot;      params.inspect    end  endendThe Nginx config is a fairly standard reverse proxy setup. The only modification that I did was to created two location blocks: one to handle the rewrite and appending a parameter to the request, and the other for URLs that did not need modification.server {    listen 80;    location /api/parp { # fix parps        rewrite (.*)/parp/(.*) /$1/$2 break;        proxy_set_body $request_body&amp;fart_noise=parp;        # other proxy directives...    }    location /api { # leave as-is        # other proxy directives...    }}Obviously I didn't come up with this conclusion immediately, I needed some kind of sketchpad / scratch space to try out the various ideas for rewriting the requests. Enter docker-compose, a handy way to join a bunch of containers together without having brain meltdown from remembering the various incantations that are the individual docker commands.Here's the docker-compose.yml I used:version: &#39;3&#39;services:  app:    build: ./app    command: [&quot;bundle&quot;,&quot;exec&quot;,&quot;rackup&quot;,&quot;--host&quot;,&quot;0.0.0.0&quot;,&quot;--port&quot;,&quot;4567&quot;]    volumes:      - ./app/:/app  proxy:    image: nginx    command: [&quot;nginx-debug&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]    volumes:      - ./proxy/nginx.conf:/etc/nginx/nginx.conf    ports:      - 8080:80    depends_on:      - appThe items you may find useful are:the proxy (Nginx) is dependant on app (Sinatra) starting up firstwe're using volumes to get the nginx container to read our nginx.conf, removing the need for a pointless Dockerfile that copies this file.You can see the whole project on GitHub.Shrewd readers will notice (in the repo) that parp isn't the item in the URL that we wanted to change. The problem was actually that there are different versions of the data set. Some view this as a change to the API therefore requiring a version tag in the URL. Others do not, which is why version was being passed as a parameter."
    },


    { 
        "title" : "Mozilla Global Sprint",

        
        
        
        "tags": [
        
            "rust",
            
        
            "mozilla"
            
        
        ],
        "href" : "2018/2018-0005-mozilla-global-sprint",
        "content" : "I wanted to share my experience of participating at Mozilla's Global Sprint 2018. The Global Sprint runs once a year and is a two day hackathon event. The projects you can work on, have a central focus of promoting a healthy Internet.At last count there were 145 projects that you could participate in during the sprint, including a certain project called: Content-o-Tron, but more on this later!Open Leaders ProgrammeA significant amount of the projects that were submitted for this year's sprint came from an excellent programme run by Mozilla called Open Leaders. This is a 14-week online course on working open, which will help you take an idea and turn it into a project that is ready to accept contributors from all over the world.I've participated in lots of Open Source projects, and even created a few of my own. So you would think I would be well versed in &quot;Working Open&quot;, but this course has taught me so much more!I plan to write a separate blog post about the programme, but I will say that if you are looking to harness the power of crowd sourcing, I recommend that you apply when applications are accepted in 2019!Content-o-TronContent-o-Tron is the evolution of an idea that I submitted as part of my application for the Open Leaders programme.Whilst on the programme, I have watched my project evolve from something that would generate more &quot;Rust&quot; content into a toolkit that can be used to grow diversity and inclusivity in communities.My personal ambition is for this project to be a stand alone resource that can referenced like Open Leaders or RailsBridge.A Sprint Finish for Content-o-TronPre-sprintA few weeks before the sprint, I started creating tasks (GitHub issues) labelled as . Previously these would've been tasks that I would fixed myself (fix typos, or write introductory text), but it was time to start sharing the work.I also spent a lot of time spamming twitter and keeping a thread of all related #mozsprint tweets.Sprint-eveSupport from friends and family is incredibly important. I had a big confidence boost, when my friend Florian told me he would be checking in occassionally and helping out with a few of the tasks!Sprint DaysI was planning to use IRC as a way to coordinate with contributors in real time (versus commenting in GitHub Issues). In the end I decided to use gitter, because I felt it would be easier to setup (it's web based) and has a good initial user experience. Also most of the sprint participants would already have GitHub accounts (which is a single sign on option in gitter).I also created a tracking issue for the Global Sprint. This is an issue where people go to register their intent to help out on the project. I then used bit.ly to create a memorable URL to this tracking issue  (bit.ly/cot-sprint-18) so it would be easier to share out during our video call &quot;pitches&quot;.It wasn't long before people started to contribute, it was interesting to see the team dynamics change as more people started to join. I like to think that because myself and Florian greeted people (in gitter) and responded to queries in a timely manner this set the tone for collaboration.Once someone had contributed to the project I immediately made them a maintainer of the repository. I wanted to remove any potential barriers, that could prevent them from getting things done. This had the added advantage of making more reviewers for PRs.When I resumed work on day two of the sprint, it was thrilling to see that people had continued to work on the project overnight. This meant I had provided enough work and that the instructions were clear.Here's a snapshot of the activity on the project during the sprint. I couldn't capture the activity for the two days, so this may have included some last minutes fixes on Sprint-eve.I did some further analysis and concluded the following occurred during the sprint.11 out of 12 pull requests closed.9 out of 23 tasks (issues) closed that were labelled .  (excluding the tracking issue)10 issues labelled as . .4 out of 10 issues closed that were labelled .  (did not require software development).2 issues that were labelled .  that required development. One of the issues (curation tool) is pending peer review, so could potentially be closed soon.ThanksI'd like to thank the following people in making this such an awesome &quot;Global Sprint&quot; experience:The Mozilla Global Sprint Team for an awesome event.The Mozilla Open Leaders Team and the cohort hosts and guest speakers. You helped me turn an idea into something that I hope will help lots of people.The Content-o-Tron contributors who helped make this a truely Open project!"
    },


    { 
        "title" : "Tunnelling TCP over DNS in 2018",

        
        
        
        "tags": [
        
            "tools"
            
        
        ],
        "href" : "2018/2018-0004-tunnelling-tcp-over-dns-in-2018",
        "content" : "I wrote this article after seeing that no one had written anything about tunnelling tcp traffic over dns since 2016.A common use of this type of tunnelling is to gain free Internet access by tunneling through a WiFi captive portal.I'd also say it's probably a good idea to get some practice before similar types of network restriction are imposed by your ISP or the state.Just like stateful packet inspection will reveal ssh tunneling over https, this is by no means a method of concealment so caveat emptor!My choice of tunneling tool is iodine. The code can be found on GitHub, the site is hosted on code.kryo.se.The setup hasn't changed along with the current version (0.7.0). Important thing to note here is that you should always have parity with the client and server version of iodine.Setting up iodine:Provision a server (via DigitalOcean et al).install iodine (ubuntu/debian) apt-get install iodinerun as a service iodined -f -c -P f00b44 10.0.0.1 t1.yourdomain.tldnote 1: f00b44 is your shared secret between client and servernote 2: just as you would with a vpn, you create a private network for your tunnel, in our case we're using thenote 3: here's my DigitalOcean referral link if you're feeling particularly generous.Create DNS records to point to your servert1ns A your_vps_ipt1 NS t1ns.yourdomain.tldtip: I use cloudflare to manage my dns, which means my DNS records propagate fairly quickly across the Internet.Install the client (mac OS)git clone https://github.com/yarrick/iodine.gitlscd iodine/lsmakemakePREFIX=/usr/local make installConnect the client to the serversudo /usr/local/sbin/iodine -d utunX -f -P f00b44 t1.yourdomain.tldnote: you need sudo to use the tunnelling network adaptor.Setup verificationTo verify setup we'll setup a ssh as a SOCKS proxy ssh -D 9999 10.0.0.1.note: we're setting up the the SOCKS proxy via our server's ip address.And then use httpbin to verify our ip address curl --socks5-hostname 127.0.0.1:9999 -L http://httpbin.org/ipWhen tunnelling you got expect a considerable loss of bandwidth, I'd only recommend using ssh over this type of tunnelling, it's just too slow to do anything else useful.random-tip: &lt;enter&gt; ~. to escape hung ssh sessions"
    },


    { 
        "title" : "Open source tools and work proxies",

        
        
        
        "tags": [
        
            "work",
            
        
            "tools"
            
        
        ],
        "href" : "2018/open-source-tools-and-work-proxies",
        "content" : "I often use a lot of open source tooling at work, initially I started with with node and npm (for our front end), and more recently python, Go and of course Rust.Unfortunately a lot of tools, expect you to have direct access to the Internet, if you're behind a work proxy most will fail to connect to their registries or pull down code from source control repositories.To make matters worse, work proxies often require Windows authentication. So rather than stick your credentials in a plain text file, you might prefer to use something like Fiddler. Fiddler is an excellent diagnostics tool for troubleshooting web apps, a bonus feature is that it also provides a local proxy (usually listen on port 8888), using your existing browser proxy settings.After that, it's just a case of configuring your tools to use this local proxy. Just be mindful that if you can't access websites like GitHub or npm in your browser, you won't magically get access this way, those restrictions will remain in place.Here's a list of ways to get your open source tool to work with this local proxy:Environment variables (env vars)Have these setup as a minimum, most tools will look of these env vars and should start working.HTTP_PROXY=http://127.0.0.1:8888HTTPS_PROXY=http://127.0.0.1:8888This works with rustup,npmnpm config set proxy http://127.0.0.1:8888npm config set https-proxy http://127.0.0.1:8888cargoedit %USERPROFILE%\\.cargo and add the following:[http]proxy = &quot;http://127.0.0.1:8888&quot;gitgit config --global http.proxy http://127.0.0.1:8888This should fix problems with golang and possibly cargo (when pull dependencies).python (Anaconda)edit %USERPROFILE%\\.condarc and add the following:channels:- defaults# Show channel URLs when displaying what is going to be downloaded and# in &#39;conda list&#39;. The default is False.show_channel_urls: Trueallow_other_channels: Trueproxy_servers:    http: http://127.0.0.1:8888    https: http://127.0.0.1:8888vscodeedit your user settings file and add  &quot;http.proxy&quot;: &quot;http://127.0.0.1:8888&quot;,  &quot;http.proxyStrictSSL&quot;: false,"
    },


    { 
        "title" : "Rust2018 - A year of talks",

        
        
        
        "tags": [
        
            "rust",
            
        
            "talks",
            
        
            "community",
            
        
            "meetups"
            
        
        ],
        "href" : "2018/rust2018-a-year-of-talks",
        "content" : "Background: This my blog post for the Rust team's request for community blog posts.Before I address my hopes for Rust in 2018, I thought I'd look back at a year of meet ups that we've organised for London's Rust user group.meet_ups.len() // The Numbers Don't LieWe had 7 talks with an average RSVP of 56 people per meet up. There were 6 &quot;Hack and Learn&quot; events (where we work on Rust together) with an average RSVP of 30 people per meet up. Finally we hosted our year end &quot;Show and Tell&quot; (lightning talks, where no demo is too small) with 22 RSVPs.That's 14 events, which given that we ran out of speakers (more on this later) was an amazing achievement!Furthermore I feel we gave our user group a choice to either listen to talks about Rust, or what I'm sure is preferable to most, to hack on Rust!I'm particularly fond of the &quot;Show and Tell&quot; because it's a relaxed format that allows people to try things out. We've had a few people go on to do main talks, so I feel it's a confidence booster.str::replace(&quot;These are a few of my Favourite Things&quot;, &quot;hing&quot;, &quot;alk&quot;)I've loved all the talks that I've heard this year, but here are a few that stood out.Jonathan Pallant (JPster) - Embedded Rust Talk (video) - This was my first example of Rust running on embedded hardware (vs talking to a server via serial). This has so far been our only talk involving physical computing, I hope it's not the last!Niko Matsakis - Compiler Internals (video) - Not our first remote talk, but certainly the most exciting because it was provided by a member of the core team. Trivia: this wasn't the first time Niko had given a talk to our group. Over a year ago he along with a sizable contingency of the core team descended upon our humble group and gave us a serious overload of Rust! You can see those videos here.Pete Hayes - Intecture 2: Tokio Drift (video) - In 2016 (video) Pete introduced us to his product Intecture. In 2017 he came back to show us his progress, which was pretty exciting, and included replacing a lot of zeromq with Tokio!Lessons learntI need to improve my time keeping; we've been close to being kicked out of the venue because we've run so late. So to address this I plan to nominate someone as time keeper.Our remote speaking set-up isn't ideal. I'd love to use Mozilla's Vidyo, but licenses don't extend to meet ups. Also Google hangout's UI is so counter-intuitive (don't @ me) that frequently new users end up not screen switching and you spend the first 5 minutes with a very personal face to face chat.I'm going to try Zoom, but if you have something you swear by that ideally allows 2 way video and screen sharing, drop me a note!On the subject of remote speakers, if you're lucky enough to have excellent A/V team who provide 2 way audio this is almost as good as having your speaker there with you. If we hadn't made use of remote speakers, I'm certain we'd have fewer meet ups in 2017. Turns out even the capital only has a finite number of people who can and want to talk about Rust!So what's in store for the talks meetup in 2018?My overarching goal is to encourage female and non-binary attendees to come to our meetups. Currently the percentage is under 3%. It would be great to get to 51%, to prove that the Rust community is as welcoming and supportive as we'd like to be.I'm going to be relying on the kindest of friends to help me achieve this, and I'll follow this post up with an outline of the actual plan.My stretch goals are:Produce more home grown speakers i.e. from our attendee list.Try out the Fishbowl (thanks Florian) and Open Space format meet ups.Find a better way to solicit feedback about the events, speakers and suggestions.ThanksBefore I end this blog, I'd like to thank the following group of people without whose help these meet ups wouldn't happen.Christian, my fellow organiser, who single handledly took on the Hack and Learn meet ups and has had equally great success.Skillsmatter for hosting us and providing video recordings. I'd like to single out the A/V team in particular who were unfazed by any of our display connectivity issues and remote talk quirks.Speakers for taking the time to talk about your passion project.The attendees for being the vital ingredient to make a meet up successful.You're still reading?Then why not enjoy our recorded talks from 2017!FebruaryJonathan Pallant - Embedded Rust Talk (video)May - Blockchains (only themed event)Parity Tech - Building blockchains at Parity Technologies (video)MaidSafe - Building the SAFE Network in Rust (video)JulyMe - Generator X: The State of Rust Static Site Generators (video)AugustAidan Hobson Sayers - Rewrite it in Rust? Some experiences in journeys from C and C++ (video)Me - Rust Language Server And You! (video)SeptemberNiko Matsakis - Compiler Internals (video)Amanieu d'Antras - Intrusive collections for Rust (video)David Harvey-Macaulay - three-rs: High-level 3D in Rust (video)OctoberDavid Dawson - Building Message and Event based APIs using Muon - Rust Edition (video)Diane Hosfelt - Attacking Rust for Rust for Fun and Profit (video)Pete Hayes - Intecture 2: Tokio Drift (video)NovemberApoorv Kothari - Ownership, the Core Principal of API Design (video)Comments or feedback?Here's the URLO thread to do this.FINhonest."
    },


    { 
        "title" : "Flocking shell",

        
        
        
        "tags": [
        
            "til",
            
        
            "flock",
            
        
            "linux"
            
        
        ],
        "href" : "2018/flocking-shell",
        "content" : "Yesterday, I had an interesting problem. My cron task spawned hundreds of copies of itself because it was blocking on a database call. If a process spawns enough times, you'll eventually run out of file descriptors and will be unable to fork more processes. To avoid further repeats, I needed to add a check to see if the script was already running and exit early.My requirements for the script in question, also requires that it be able to spawn a specific instance. Instance in this case, could mean connecting to a different database. The important takeaway is that each instance, must be allow a spawn single copy of itself.I could've gone down the route of using creating a PID or lock file (storing the current process id of the script), checking if the current process and the PID file matched and exiting if not.Instead I fancied trying something different and according to StackOverflow flock was a popular choice.Here's a snippet of how to enable file locking in your scripts.# how to allow the script multiple times for different instancesreadonly LOCKFILE=&quot;${LOCKFILE_DIR}/${PROGNAME}-${INSTANCE}.lock&quot;# to avoid command block, link file descriptor (auto incremented) to our lock fileexec {lock_fd}&gt;&quot;$LOCKFILE&quot;# early exit if instance is already runningflock -n ${lock_fd} || exit 1The funny notation {lock_fd} is an auto-incrementing named file descriptor which doesn't appear until bash 4.1.x.x (so you're out of luck Mac users). To add the Mac woes, flock isn't bundled with Mac, but someone's created a cross platform version with the same name.To prove my script no longer spawned multiple copies I wrote the following script (safe-driver.sh):#!/bin/bashclearfor i in $(seq 3)do     (         echo &quot;&gt; BEGIN FOO $i&quot;        safe.sh FOO        echo &quot;&gt; END FOO $i exit code: $?&quot;    ) &amp; doneif [ ! -z &quot;$IN_DOCKER&quot; ]; then    sleep 1 # allow scripts to run (needed for docker)fiprintf &quot;\\n\\njobs running (should only see one process running)\\n&quot;jobs -lprintf &quot;\\n\\nlist file locks\\n&quot;lsof /tmp/safe*.lockif [ ! -z &quot;$IN_DOCKER&quot; ]; then    printf &quot;\\n\\npausing, press any key to return early\\n&quot;    read -rfiReferencesElegant locking of bash program blog post - I cribbed the idea of not running flock as a command block from Kfir's post, but I drew the line with how the code was organised. Where possible I try to avoid imposing coding style from other languages. I also still think bash can be consumed by two parties operational staff and developers, I would prefer to cater for ops since they usually end up looking after these scripts.&lt;/soapbox&gt;exec examples from bash-hackers.org - This was my first time to use exec in anger and I think it helped me understand the role the file descriptor played in my flock script.Advanced Bash-Scripting Guide (Special Characters - This is my goto resource for searching for various symbols and glyphs often used blindly in bash. In particular I used this to find out the proper name for () (command block).UpdatesYou may have seen an earlier post (on the 10th), which I withdrew because I didn't feel I had solved the problem sufficiently and there was a misunderstanding of how automatic file descriptor allocation works."
    },


    { 
        "title" : "Porting python turtle examples to turtle.rs",

        
        
        
        "tags": [
        
            "turtle",
            
        
            "rust",
            
        
            "python"
            
        
        ],
        "href" : "2017/porting-python-turtle-examples-to-turtle-rs",
        "content" : "I've been tinkering around with the Rust version of Turtle graphics. Turtle graphics, was a key feature of the programming language Logo, and has frequently been ported to other programming languages as a visual way to teach programming.I read somewhere that the author Sunjay Varma of Rust port of Turtle was inspired by python's own module.There's a few minor cosmetic changes pendown() vs pen_down(). So I decided to have a go at running existing python examples and I came across this code from the module documentation.from turtle import *color(&#39;red&#39;, &#39;yellow&#39;)begin_fill()while True:    forward(200)    left(170)    if abs(pos()) &lt; 1:        breakend_fill()done()The only item that threw me was the abs(pos()). pos() is an alias to the position method which returns the cartesian coordinates (x,y). I figured some form of special handling was taking place.A quick search of the code.$ python3 -v*lots of irrelevant noise*&gt;&gt;&gt; import turtle# /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk/turtle.pyc matches /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk/turtle.pyA quick peek at the source code identified the position method returns a class called Vec2D, which in turn has a definition for abs' behaviour within this class.def __abs__(self):    return (self[0]**2 + self[1]**2)**0.5My geometry is too sketch to guess at why you'd want to square the pair of cartesian coordinates, add the two numbers together. Finally exponentiation the result by 0.5, but there you go, this is what absolute value of a pair of cartesian coordinates should be.The equivalent Rust code would look something like this:fn abs(point : &amp;Point) -&gt; f64 {    (point[0].powf(2.0) + point[1].powf(2.0)).powf(0.5)}The only method to address is color which in it's two parameter form sets the pen and fill colour.fn color(turtle: &amp;mut Turtle, pen_color: &amp;str, fill_color: &amp;str) {    turtle.set_pen_color(pen_color);    turtle.set_fill_color(fill_color);}Here's the code in it's entirety:extern crate turtle;use turtle::{Turtle,Point};fn main() {    let mut turtle = Turtle::new();    color(&amp;mut turtle, &quot;red&quot;, &quot;yellow&quot;);    turtle.begin_fill();    loop {        turtle.forward(200.00);        turtle.left(170.00);        if abs(&amp;turtle.position()) &lt; 1.0 {            break;        }    }    turtle.end_fill();}fn abs(point : &amp;Point) -&gt; f64 {    (point[0].powf(2.0) + point[1].powf(2.0)).powf(0.5)}fn color(turtle: &amp;mut Turtle, pen_color: &amp;str, fill_color: &amp;str) {    turtle.set_pen_color(pen_color);    turtle.set_fill_color(fill_color);}The result is fairly close!"
    },


    { 
        "title" : "London Perl Workshop 2017",

        
        
        
        "tags": [
        
            "perl",
            
        
            "lpw",
            
        
            "rust"
            
        
        ],
        "href" : "2017/london-perl-workshop",
        "content" : "Almost a month a go, around the same time as MozillaFest, NeilBowers put out a request for a Rust talk viatwitter.I would love someone to give an intro talk on #rustlang at #lpw201720 minute intro from anyone?— Neil Bowers (@neilbowers) October 27, 2017This was my first request for a Rust talk at a conference! I'm glad I didn't think too hard about that fact, as I suspect I would've been more freaked out!The talk submission process, as well as the communications leading up to the event were excellent. One of the nicest emails, was tips for new speakers, it was like having a distilled version of Scott Hanselman's speaking tips.I managed to attend two talks:Abigail's [Regexp Mini-Tutorial: Character Classes]Sue Spence's [Spiders, Gophers &amp; Butterflies]I was surprised at how much Regexp's character classes had expanded to accommodate unicode characters. I thought the class properties were very descriptive. It was nice to see Perl continue to evolve.Sue's talk was about concurrency and the differences between implementation of a webspider in Go and Perl 6. It inspired me to have a go in Rust to see how comparable to the other two languages. I found Perl 6's syntax easier to read.My talk was well attended, I managed to do my own take on the famous &quot;Control/Safety&quot; spectrum of programming languages. A few days earlier, I tested the talk on our local Rust user group and got invaluable feedback. Despite having to rejigging the slides I still managed to deliver the Introduction to Rust in time!The people who I met at the conference were lovely. The AV staff and volunteers were incredibly helpful. I'd recommend submitting a talk for London Perl Workshop 2018!My deck can be found at SpeckerDeck.The video will follow shortly, I will update this post when it's available."
    },


    { 
        "title" : "MozFest 2017 Rust Resources",

        
        
        
        "tags": [
        
            "rust",
            
        
            "rustlang",
            
        
            "mozfest",
            
        
            "mozilla"
            
        
        ],
        "href" : "2017/mozfest-2017-rust",
        "content" : "Here's the websites I've been touting at MozFest:The API documentation for the rand crate, to demonstrate the amazing detail and functionality you can add to your crate's documentation.The documentation and their location in the source code.Platforms supported by Rust (OS, chipset, etc)PodcastsNew RustaceanRequest for ExplanationRusty SpikeFriends of Rust (Organizations using Rust in Production)community.rs24 days of Rust - one of the best curated lists of Rust crates and features."
    },


    { 
        "title" : "Remote speaking tips",

        
        
        
        "tags": [
        
            "talks",
            
        
            "publicspeaking",
            
        
            "hangout",
            
        
            "remote",
            
        
            "livecoding",
            
        
            "demo"
            
        
        ],
        "href" : "2017/remote-speaking-tips",
        "content" : "On Monday I did my first remote talk (for the RustEdinburgh user group). I thought I'd sharemy experience to help others who want to do the same.tl;dr - your check list:Get a good quality headset, don't use the internal microphone on your computer.Do a mirror (selfie) test, make sure there's no embarrassing stuff in the background.Make more slides, allow the information to trickle through to re-enforce concepts.Avoid information dense slides.Do a test video conference call beforehand.Pre-bake your demos unless you're a live coding veteran.Don't ramble, short sound bites are more memorable.Practice.GearGet a decent headset, do not use your internal microphone on your computer! Iborrowed my better half's pair of HyperX CloudII,they're her gaming headphones, but they also function excellently for videoconferencing.An acceptable low-fi alternative is a phone hands free kit. The ones that comebundled with iPhones are surprisingly good for this task.SetupIf this is your first time to do video conferencing, then do a mirror or selfietest. In google hangout you can do this by clicking on the video call icon.Look around you, is there anything that you wouldn't want to appear in thebackground? Our home office is also our guest bedroom and where we keep ourclothes dryer - you get the idea.Bonus points if you can stick cool posters or a talk related paraphernalia(mascots and the like) in the background!Deck DesignThis is the area that I'm still trying to master, so I will prolly revisit in aseparate post, but here's tips I was given by more experienced speakers.Keep the detail on each slide, light, but also have more slides to re-enforcethe concepts you're trying to get across. You no longer have the advantage ofseeing the audience reaction i.e. how well they're grasping the subject matter.So drip feed the information, gradually.Google HangoutsAt the moment I'm still using Google Hangout to conduct my remote talks and tofacilitate remote speakers for the London Rust user group. Provided bothparties have Chrome, it's zero software install.However, I do have concerns, in particular the lack of multi-display support(more on this in a moment) and the inability to increase the size of the otherparty's video window.Multi display (or lack of)When doing talks in person, nearly all A/V equipment allow you to use theprojector/TV as an additional display. This allows me to configure Keynote (Macpresentation tool) in presenter mode. The slides appear on the project/TV asyou would expect and on your own computer's display you can see the timeelapsed, next slide and presenters notes.Google hangout allows you to share your screen in two modes: full screen orapplication. Neither modes are multi display aware. This means Keynote willonly display the slides, no presentation mode! Imagine my surprise when Idiscovered as I start my talk!Luckily I had my laptop as a second computer (for my live coding script). So afew seconds later I had the presentation on this computer with notes. Thealternative would be to use Keynote on your iOS device as a presentationremote. You can then configure it to display your presentation notes alongsidethe current slide.The inability to increase the size of the other party's videoThis means you can't see the audience, well not well enough to know if they'refollowing your talk.Live CodingDon't do it unless you can already touch type and talk to someone at work! It'sbetter to pre-record you live coding, then you can edit out pauses. You'll alsofind that providing commentary is easier, when you're not worry about what totype next.If you do decide you want to do live coding, keep a script of what you want toshow. If it makes sense have the code snippets handy.My talk about was the Rust Language Server and it's integration into VisualStudio Code, so I needed to make errors to demonstrate functionality. So codesnippets wouldn't have been useful.If you have a suitable screen recording that also records your microphone, useto practice. Once the lengthy pauses have stopped you're be ready!I did do live coding, but I've done this talk twice. And I also practiced thescript everyday until I did the talk.PreparationIf possible do a test call before the day of the talk. This gives you time toiron out any technical glitches. Also expect to do another setup call a fewminutes before you talk, your hosts will usually want this.The Rust Edinburgh organisers also asked for a copy of my slides. The plan wasthat if the internet connection dropped off, they would call my phone and theywould go through the slide on my behalf.PerformanceRambling in general is a bad idea in talks. In remote talks it can spoil yourperformance. Remember you can't see your audience, if you could see the yawnsor people staring at their phones you would know you're rambling.Finally this one is a given if you want to do talks: practice makes perfect!-- Don't forget to take an audience selfie!"
    },


    { 
        "title" : "National Novel Generation Month 2016 Reflection",

        
        
        
        "tags": [
        
            "procgen",
            
        
            "nanogenmo",
            
        
            "rust",
            
        
            "bash"
            
        
        ],
        "href" : "2017/nanogenmo-2016-reflection",
        "content" : "National Novel Generation Month aka NaNoGenMo is a month long contest to write code to generate a novel of 50k+ words.Before I start talking about my plans for this year, I thought it would be a good idea to reflect on my previous efforts. In last year's contest I created two novels(!):Banned Books CensoredThe statement of intent on NaNoGenMo: github.com/NaNoGenMo/2016/issues/94.I redacted the following books (via Project Gutenberg) listed in Anne Haight's Banned Books (booked banned by authorities or religious organisations at some point in time). The numbers relate to the Project Gutenberg book ID:The_Clouds by Aristophanes (2562)The Birds by Aristophanes (3013)The Analects of Confucius by Confucius (3100)The Analects of Confucius by Confucius (3330)The Odyssey_by_Homer (3610)The Analects of Confucius by Confucius (3610)Lysistrata by Aristophanes (7700)The code to do these novels can be found at github.com/booyaa/fuckcensorship. The language I used was Bash. The main crux of the code is to replace non-whitespace characters with Unicode FULL BLOCK character (█):for book in $FOLDER_IN/*.txt; do    echo &quot;Processing $book...&quot;    FILE_OUT=$FOLDER_OUT/$(basename &quot;$book&quot; .txt).censored.txt    $SED_EXEC  &#39;s/\\S/\\xE2\\x96\\x88/g&#39; &lt; &quot;$book&quot; &gt; &quot;$FILE_OUT&quot;doneHere's a sample of The Odyssey.███████████████████████ ██ ██ ████ ███ ██████ ██ ██████████ ██ █████████ ██ █████████████ ██ ██ ███████ ████ ████ ██ ██ ███████ █████ ███ ███ ███████ █████ ██ ████ ███ ████ ███████ ███████████ ██████ ████ ███ ███████████ █████████ ██ ███ ██████████ ██ ████ ███████████ ███████ █████████████ █████████ █████ █████████ ██████████ █████████ ██ ████ ████████ ███ ███████ ███ ███████ █████ █████ ████ ██ ██ ██████ ██ ██████ █████ ██████████ █████████ █████ ██ ███ ████ ██ ██ █████ █████████ ███████ ██ ████████The collection of censored books can be found on github.com/booyaa/bannedbookscensored/tree/master/censoredA book of NovemberThis novel was a simple concept, you need to write 1,666 words a day to hit the 50k target for NaNoWriMo. So why not write chapter consisting of the number of the day for the month i.e. the 1st of the month would be one?A book of NovemberStatement of intent on NaNoGenMoThe code to create this novel can be found at github.com/booyaa/nanogenmo2016. The language I used to write the code was Rust. The code snippet I'm particular proud of is my sentence length variation func.fn get_sentence(word: &amp;str) -&gt; (String, u64) {    let mut rng = rand::thread_rng();    let faces = Range::new(1, 9);    let num = dice(&amp;faces, &amp;mut rng);    let sentence_sizes = match num {        1 | 4 | 7 =&gt; 5,        2 | 5 | 8 =&gt; 7,        3 | 6 | 9 =&gt; 13,        _ =&gt; 0,    };    let mut sentence = String::new();    for i in 0..sentence_sizes {        sentence.push_str(&amp;format!(&quot;{} &quot;, word));    }    sentence = sentence.trim().to_string();    sentence.push_str(&quot;. &quot;);    (sentence, sentence_sizes)}Admittedly the effect is a bit subtle to be noticed the first time around. Also in reflection I should've capitalized the start of the sentence.Here's a sample of the novel (brace yourself)oneone one one one one one one one one one one one one. one one one one one. one one one one one one one. one one one one one one one. one one one one one. one one one one one. one one one one one. one one one one one one one. one one one one one. one one one one one one one. one one one one one one one. one one one one one one one. one one one one one one one. one one one one one one one one one one one one one. one one one one one one one one one one one one one. one one one one one one one. one one one one one. one one one one one one one one one one one one one.SummaryI didn't spend the whole month writing code to write a novel, but the exercise was a lot of fun. Were my novels any good? No not really, they should be filed under the joke or stunt category.However, don't let my efforts convince you that it's not a worth while project. Here's some of my favourites from last year:Captain's Log - Ronseal: eoinnoble.github.io/captains-log/output/captains-log.htmlDear Santa - Search twitter for requests to add to Santa's list 2016-11-25 aka Black FridayAnnales - World history generator github.com/spikelynch/annales/blob/master/output/annales.pdfTiny Tarot Stories - A story generated through tarot readings github.com/enkiv2/misc/blob/master/nanogenmo-2016/tinyTarotStories.md"
    },


    { 
        "title" : "Alexandria PL/SQL Utility Library",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "plsql",
            
        
            "ooxml",
            
        
            "microsoft"
            
        
        ],
        "href" : "2017/alexandria-plsql-utility-library",
        "content" : "Imagine if BatMan was an Oracle DBA, his utility belt would be the Alexandria PL/SQL Utility Library.How to install Microsoft Office document parsers (OOXML)AssumptionsThis has been tested on 12c r1, I imagine it should still work for 11G.SCHEMA_ALEX_INSTALL - This is the schema you have created that will host the Alexandria library.Using SQLPlus or Oracle SQL Developer (in SQLPlus mode)Script have installed to a path that SQLPlus can findInstallation instructionsAs sys or similiargrant execute on dbms_random to SCHEMA_ALEX_INSTALL;grant execute on dbms_lob to SCHEMA_ALEX_INSTALL;grant execute on util_file to SCHEMA_ALEX_INSTALL;grant create any directory to SCHEMA_ALEX_INSTALL;As SCHEMA_ALEX_INSTALLset scan off;prompt Creating types@types.sqlprompt Creating MICROSOFT package specifications@../ora/string_util_pkg.pks@../ora/zip_util_pkg.pks@../ora/xml_util_pkg.pks@../ora/sql_util_pkg.pks@../ora/file_util_pkg.pks@../ora/debug_pkg.pks@../ora/xml_builder_pkg.pks@../ora/xml_dataset_pkg.pks@../ora/xml_stylesheet_pkg.pks@../ora/xml_util_pkg.pks@../ora/ooxml_util_pkg.pksprompt Creating MICROSOFT package bodies@../ora/string_util_pkg.pkb @../ora/zip_util_pkg.pkb@../ora/xml_util_pkg.pkb@../ora/sql_util_pkg.pkb@../ora/file_util_pkg.pkb@../ora/debug_pkg.pkb@../ora/xml_builder_pkg.pkb@../ora/xml_dataset_pkg.pkb@../ora/xml_stylesheet_pkg.pkb@../ora/xml_util_pkg.pkb@../ora/ooxml_util_pkg.pkbprompt Done!Verifying the installationAs SCHEMA_ALEX_INSTALLHOST md d:\\temp\\devtest -- Un*x users: mkdir /path/to/devtestCREATE DIRECTORY devtest_temp_dir AS &#39;d:\\temp\\devtest&#39;; -- Un*x SET SERVEROUTPUT ONDECLARE    l_blob    BLOB;    l_props   ooxml_util_pkg.t_xlsx_properties;BEGIN    debug_pkg.debug_on;    l_blob := file_util_pkg.get_blob_from_file(&#39;DEVTEST_TEMP_DIR&#39;,&#39;hello_excel.xlsx&#39;);    l_props := ooxml_util_pkg.get_xlsx_properties(l_blob);    debug_pkg.printf(        &#39;title = %1,modified = %2,creator = %3,application = %4&#39;,        l_props.core.title,        l_props.core.modified_date,        l_props.core.creator,        l_props.app.application    );END;/"
    },


    { 
        "title" : "Oracle tips",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "plsql",
            
        
            "tips"
            
        
        ],
        "href" : "2017/oracle-tips",
        "content" : "Hiding user inputSometimes you need to keep something secret (shoulder surfing), this will only work in SQL/Plus or Oracle SQL Developer (F5/Run script mode aka broken SQL/Plus mode).SET SERVEROUTPUT ONSET VERIFY OFFACCEPT sekrit PROMPT &#39;enter a secret (warning we&#39;&#39;re going to print it on screen!)&#39; HIDEBEGIN    dbms_output.put_line(&#39;sekrit: &#39; || &#39;&amp;sekrit&#39;);END;/If you ran the script correctly, the input dialogue will echo stars * instead of your &quot;secret&quot;.If you can see the data you're entering, you ran Oracle SQL Developer statement mode (CTRL-ENTER).SessionsFindingSET LINESIZE 140SET PAGESIZE 50COL sid FORMAT a5COL serial FORMAT a5COL username FORMAT a30COL osuser FORMAT a30COL machine FORMAT a30COL client_ip FORMAT a20SELECT    sid,    serial# AS serial,    osuser,    username,    machine,    logon_time,    utl_inaddr.get_host_address(regexp_replace(machine,&#39;^.+\\\\&#39;) ) AS client_ipFROM    v$sessionWHERE        username IS NOT NULL    AND        status &lt;&gt; &#39;KILLED&#39;;Killingalter system kill session 'sid,serial#';Code generated exampleSELECT     &#39;alter system kill session &#39;&#39;&#39; ||sid||&#39;,&#39;||serial#|| &#39;&#39;&#39;;&#39; as sqlFROM    v$sessionWHERE    username = &#39;VICTIM&#39;;"
    },


    { 
        "title" : "C Sharp and .NET tips",

        
        
        
        "tags": [
        
            "c#",
            
        
            "csharp",
            
        
            "dotnet",
            
        
            ".net",
            
        
            "dotnetcore"
            
        
        ],
        "href" : "2017/csharp-dotnet",
        "content" : "NUnit TestCases with instances of a typepublic IEnumerable&lt;TestCaseData&gt; CanParseAsThingyTestCases{    get    {        Setup();        yield return new TestCaseData(@&quot;foo&quot;, new Thingy { name = &quot;foo&quot; });        yield return new TestCaseData(@&quot;bar&quot;, new Thingy { name = &quot;bar&quot; });            }}[TestCaseSource(&quot; CanParseAsThingyTestCases&quot;)]public void CanParseAsThing(string input, Thingy expected){...source: https://stackoverflow.com/a/4230328/105282RegExA somewhat convoluted example to demonstrate to avoid magic strings (ish)/string pattern = @&quot;^(?&lt;WANT&gt;Foo)bar$&quot;;string input = @&quot;Foobar&quot;;Regex r = Regex(patern, RegexOptions.SingleLine);Match m = r.Match(input);foreach (string groupName in r.GetGroupNames()) {  if (groupName != &quot;0&quot;) {    Console.WriteLine($&quot;Found {m.Groups[groupName].Value} in {groupName}&quot;);  }}Note to self:Turn this into a useful examplehttps://codereview.stackexchange.com/a/6962/2500https://stackoverflow.com/a/10417114/105282A gitignore for dotnet coresource: https://gist.github.com/booyaa/db187f5555afdba82d371b76119920c5# miscellany and obsolete artifact*.swp*.*~project.lock.json.DS_Store*.pyc# Visual Studio Code.vscode# Build results[Bb]in/[Oo]bj/"
    },


    { 
        "title" : "A developer on-boarding guide for Rust",

        
        
        
        "tags": [
        
            "learning",
            
        
            "rust"
            
        
        ],
        "href" : "2017/learning-rust-meta",
        "content" : "Background: We needed a meta reference for our study group event!Setting up your IDE/Code editor for RustNick's RLS showcase blog postbooyaa's RLS talk - if you want to see the past London group's talks you can see them here.Neovim plugin that support RLSCode completion in emacs using emacs-racerUseful resourcesThe Rust forum aka URLOThe Rust Community Team Website[The Request for Explanation Podcast(https://request-for-explanation.github.io/podcast/) - A weekly discussion of Rust RFCs.The official Rust youtube channelHack and Learn Group InfoOur siteChatroom - Looks like we've reached max connections at London! We'll try to find a better solution for next time!"
    },


    { 
        "title" : "Troubleshooting the Rust Language Server",

        
        
        
        "tags": [
        
            "rls",
            
        
            "vscode",
            
        
            "help"
            
        
        ],
        "href" : "2017/troubleshooting-rls",
        "content" : "To understand how to troubleshoot the Rust Language Server (RLS), it helps to know what RLS is and how the components interact.RLS is a Rust implementation of the Language Server Protocol (LSP). LSP is based on the client server architecture, and simplifies the way code editors and IDEs interact with a programming language.The client in this instance is the official RLS extension. The server is rls which is another Rust command line tool like rustfmt, racer and rustsym. The extension is responsible for setting up and starting rls.If you want to find out more I recommend checking out @nrc's introductory blog post: What the RLS can do.Whilst this post is tailored towards the VS Code extension, where possible I'll let you know when you can run this against your own setup.The up to date mantraAlways have the latest version of Rust nightly and RLS. rustup updateKeep Visual Studio Code and the Rust RLS extension up to date.Increase logging levelTo turn on the fire hose:RUST_LOG=rls=debug code .To reduce the noise you can drop down to informational:RUST_LOG=rls=info code .This tip assumes you have Visual Studio Code in your path so it can be launched from the command line.Diagnostics will appear in the Output panel for the Rust Language Server (the drop down to the right of the panel).This can also be used for other editors, just replace code with your own editor.Additional configuration settingsIn your user or workspace settings (settings.json) add the following  configuration parameters:{    // Includes standard error from RLS in the Output panel. This isn&#39;t     // particular useful if you&#39;ve already enabled logging at debug level.    &quot;rust-client.showStdErr&quot;: true,        // This will also log everything that appears in the Output panel to a     // log file in the root of your workspace.    &quot;rust-client.logToFile&quot;: true,}Both of these will require a restart of the editor. If your LSP client implements theworkspace/didChangeConfiguration method, then add the same keys to your client's configuration file.Alternatively send the abovementioned method and that example json fragment to RLS.Where to go if you need more help?@nrc has just written the definitive guide to debugging and troubleshootingRLS. I would recommend you also visit this article.As always with any good open source project, help is always available via a new github issue. Alternative if you use irc, you can ask in #rust-dev-tools on the Mozilla network."
    },


    { 
        "title" : "Regular Expressions Miscellany",

        
        
        
        "tags": [
        
            "regularexpression",
            
        
            "regex",
            
        
            "oracle",
            
        
            "notepad++"
            
        
        ],
        "href" : "2017/message-queue-miscellany",
        "content" : "Oracle related regular expressionsregexp_count and regexp_replace functionsLet say you have a column that contains source for an Oracle object. We want to extract  documentation hidden in sql multiline comment block /* .... */, it's been wrapped in a pair of @@ for easy identification. ID  source                                 documentation  1   /@@treasure@@/                       treasure           select foo from bar                                   2   select fizz from buzz                                The little gotcha is that there's a linebreak between the documentation and the actual code.Whilst not regular expression as per se you want to use translate to remove any type of non printing whitespace (tabs, line breaks and carriage returns).translate(source, CHR(10)||CHR(11)||CHR(13), ' ')This will allow you to supply regexp_replace with the following regular expression '^..@@(.*)@@(.*)' and extract the documentation.Here's the entire solution with a common table expression to provide some test data.COL source FORMAT a30COL documentation FORMAT a30WITH data AS (    SELECT        &#39;/*@@treasure@@*/&#39;         || CHR(10)         || &#39;select foo from bar&#39; AS source    FROM        dual    UNION ALL    SELECT        &#39;select fizz from buzz&#39;    FROM        dual) SELECT    source,        CASE            WHEN regexp_count(source,&#39;^..@@(.*)@@(.*)&#39;) &gt; 0 THEN regexp_replace(                translate(                    source,                    CHR(10)                     || CHR(11)                     || CHR(13),                    &#39;    &#39;                ),                &#39;^..@@(.*)@@(.*)&#39;,                &#39;\\1&#39;            )        END    AS documentationFROM    data;The only addition is a check to see if the regular expression was matched against the source field so we only processed those columns and return no data in the documentation field.I used this to enhance this handy ORDS to Swagger script to provide documentation in the description tags rather than the source code (which it to fail validation).notepad++Creating fake data as an in-memory SQL tableyou want to turn thisJEB JIBBLY 1234 3BOBBY TABLES 2314 1WHISKY JACK 2241 2into thisWITH FOO AS ( SELECT &#39;JEB JIBBLY&#39; &quot;FULL_NAME&quot;, &#39;1234&#39; &quot;PIN&quot;, &#39;3&#39; &quot;FAILED_ATTEMPTS&quot; FROM DUAL   UNION ALL  SELECT &#39;BOBBY TABLES&#39; &quot;FULL_NAME&quot;, &#39;2314&#39; &quot;PIN&quot;, &#39;1&#39; &quot;FAILED_ATTEMPTS&quot; FROM DUAL  UNION ALL  SELECT &#39;WHISKY JACK&#39; &quot;FULL_NAME&quot;, &#39;2241&#39; &quot;PIN&quot;, &#39;2&#39; &quot;FAILED_ATTEMPTS&quot; FROM DUAL)SELECT * FROM FOO;  notepad++ regexpfind what: (.*)\\s+(\\d{4})\\s(\\d+)replace with: SELECT '\\1' &quot;FULL_NAME&quot;, '\\2' &quot;PIN&quot;, '\\3' &quot;FAILED_ATTEMPTS&quot; FROM DUAL UNION ALLmake sure you have enabled regular expression in search mode!then obviously you'll have to format and remove unnecessary UNIONs"
    },


    { 
        "title" : "London Rust User Group Meetup No. 15",

        
        
        
        "tags": [
        
            "rust",
            
        
            "meetup",
            
        
            "usergroup"
            
        
        ],
        "href" : "2017/london-rust-15",
        "content" : "InterwebsFixed as of 0.3.2 2017-08-15 on #Rocket&lt;Sergio&gt; Quick announcement: latest Rust nightly breaks Rocket&#39;s lints. Please use a nightly between 2017-08-10 and 2017-08-13 while the issue is resolved.URLO: VendoringThis week in Dev Tools No. 1AnnoucenmentsRust Study Group and Hacking On Meetup No. 2RequestsTalk Spots (add comment on the related issue if you want to talk)September 20th - 2 lightning talks (5-15m) availableMore dates - We're pre-booked with Skillsmatter up to the end of the year!JobsSee me after the talks if you want to advertise!"
    },


    { 
        "title" : "Message Queue Miscellany",

        
        
        
        "tags": [
        
            "mq",
            
        
            "messagequeue"
            
        
        ],
        "href" : "2017/message-queue-miscellany",
        "content" : "This is a bit of a hodge podge of message queue notes. Very IBM centric at the moment.Command line toolsCheatsheetSET MQSERVER=CHANNEL.NAME/TCP/HOST OR IP ADDRESS(PORT)AMQSPUTC QUEUENAME QUEUEMANAGER &lt; SOME\\FILE\\CALLED\\HELLO.TXTAMQSGETC QUEUENAME QUEUEMANAGERDisplay library versionWin (or c:\\windows\\assembly)C:\\IBM\\WebSphere MQ\\dspmqver -iUnix (should be in pathdspmqver -isource: http://www-01.ibm.com/support/docview.wss?uid=swg21621707tag: mq , .net , dllRFHUtilcQueue Manager Name: CHANNEL NAME/TCP/HOSTNAME(PORT)"
    },


    { 
        "title" : "Using Rust with Visual Studio Code",

        
        
        
        "tags": [
        
            "rust",
            
        
            "vscode",
            
        
            "tips"
            
        
        ],
        "href" : "2017/rust-vscode",
        "content" : "Which extension?First off if you can, you should be using the Rust Language Server (RLS) extension. Yes it's beta, but the user experience has been the best I've had, outside of in-house language support by giants like Microsoft and JetBrains! The extension will even install dependencies for you if you have rustup installed!If you didn't want to use RLS, then the alternative is to install various Rust related tools (racers, rustfmt and rustsym) manually. The only Rust extension that support non-RLS is Kalita Alexey's.Whilst we're on the subject of Kalita's extension, this was a fork of the RustyCode extension which was no longer being actively maintained.The biggest draw this extension at the time was that it was available on the Visual Studio Marketplace, where as the RLS team extension had to be manually installed via git.It will be interesting two see how the two active extensions progress.If you want a proper whistle stop tour of RLS I recommend you pop over to @nrc blog as he's done a thorough job of it in this post.DebuggingI'm ashamed to admit that I still find setting up debugging Rust a bit of a black art if you're not using gdb. However thanks to the LLDBDebugger extension it's become a little bit easier.The only bit that caught me out was the launch.json boiler plate code (see below for a sample), specifically what would be the correct value for program key. This is the path to your debug binary.{    &quot;version&quot;: &quot;0.2.0&quot;,    &quot;configurations&quot;: [        {            &quot;type&quot;: &quot;lldb&quot;,            &quot;request&quot;: &quot;launch&quot;,            &quot;name&quot;: &quot;Debug&quot;,            &quot;program&quot;: &quot;${workspaceRoot}/&lt;your program&gt;&quot;,            &quot;args&quot;: [],            &quot;cwd&quot;: &quot;${workspaceRoot}&quot;        }    ]}So if the binary you wish to debug is called foo, your value for the program key would look like this:{    &quot;program&quot;: &quot;${workspaceRoot}/target/debug/foo&quot;}note: I've omitted the rest of the json keys that don't change for the sake of brevity.If you wanted to keep things generic and only compile a binary that matches the cargo folder name, you could use ${workspaceRootFolderName} variable substitution.{    &quot;program&quot;: &quot;${workspaceRoot}/target/debug/${workspaceRootFolderName}&quot;,}If you're interested in what other variables substitutions are available the Visual Studio Code Debuggerguide has a handy list.One last option to enable is sourceLanguages with the value of &quot;rust&quot;, this option enables visualisation of built-in types and standard library types i.e. you can peek into the contents of a Vec etc.Here's a complete example of the launch.json for reference.{    &quot;version&quot;: &quot;0.2.0&quot;,    &quot;configurations&quot;: [        {            &quot;type&quot;: &quot;lldb&quot;,            &quot;request&quot;: &quot;launch&quot;,            &quot;name&quot;: &quot;Debug&quot;,            &quot;program&quot;: &quot;${workspaceRoot}/target/debug/${workspaceRootFolderName}&quot;,            &quot;args&quot;: [],            &quot;cwd&quot;: &quot;${workspaceRoot}&quot;,            &quot;sourceLanguages&quot;: [&quot;rust&quot;]        }    ]}"
    },


    { 
        "title" : "Rust Language Server and Visual Studio Code",

        
        
        
        "tags": [
        
            "rls",
            
        
            "rust",
            
        
            "vscode",
            
        
            "languageserverprotocol"
            
        
        ],
        "href" : "2017/vscode-rls",
        "content" : "Click here to skip the history lesson and go straight to the tips.I first heard about the Rust Language Server (RLS), via Phil Dawes' Racer talkat the 4th London Rust UserGroup.Immediately I knew the significance of this strategy and how it would reducethe friction required to get Rust working on the vast array of editors andIDEs.This was also around the same time I switched from using vim to Visual StudioCode as my primary editor. Until code landed, I had to make do with the nowdefunct Rusty Code which like all other editor add-ons requires a fair bit ofwork to have a coding environment that is Rust savvy.As soon as RLS alpha came out, I happily gave it a go (along with it'sreference Visual Studio Code extension). Eventually we'd see RLS move into therust-lang-nursery repo (where great ideas get incubated) and also becomeanother rustup component (like src).I would flit between using the reference extension and Kalita Alexey's fork,waiting for the day when the reference extension also got added to the VisualStudio Marketplace.Finally the big day has come and the reference Visual Studio Code extension hasnow landed in the Marketplace! So getting Rust to work with Visual Studio Codeis super simplified. In fact I think if you already have a recent version ofrustup, installing theextensionwill trigger the installation of RLS automatically!So you can imagine over time you start to gather a lot of Rust extension leavings...I'd recommend you go through each of your workspace or user (global) settingsfile and remove anything rust related.Rusty Code{    &quot;rust.racerPath&quot;: null, // Specifies path to Racer binary if it&#39;s not in PATH    &quot;rust.rustLangSrcPath&quot;: null, // Specifies path to /src directory of local copy of Rust sources    &quot;rust.rustfmtPath&quot;: null, // Specifies path to Rustfmt binary if it&#39;s not in PATH    &quot;rust.rustsymPath&quot;: null, // Specifies path to Rustsym binary if it&#39;s not in PATH    &quot;rust.cargoPath&quot;: null, // Specifies path to Cargo binary if it&#39;s not in PATH    &quot;rust.cargoHomePath&quot;: null, // Path to Cargo home directory, mostly needed for racer.                                 // Needed only if using custom rust installation.    &quot;rust.cargoEnv&quot;: null, // Specifies custom variables to set when running cargo. Useful for                            // crates which use env vars in their build.rs (like openssl-sys).    &quot;rust.formatOnSave&quot;: false, // Turn on/off autoformatting file on save (EXPERIMENTAL)    &quot;rust.checkOnSave&quot;: false, // Turn on/off `cargo check` project on save (EXPERIMENTAL)    &quot;rust.checkWith&quot;: &quot;build&quot;, // Specifies the linter to use. (EXPERIMENTAL)    &quot;rust.useJsonErrors&quot;: false, // Enable the use of JSON errors (requires Rust 1.7+).                                  // Note: This is an unstable feature of Rust and is still in the process of being stablised    &quot;rust.useNewErrorFormat&quot;: false, // &quot;Use the new Rust error format (RUST_NEW_ERROR_FORMAT=true).                                      // Note: This flag is mutually exclusive with `useJsonErrors`.}Kalita's Rust fork of the reference extensionI used the RLS integration rather than the legacy setup. The legacy setuprequires you to install rust's friends like rustfmt and racer manually.{    &quot;rust.rls&quot;: {         &quot;executable&quot;: null, // The path to an executable to execute         &quot;args&quot;: null, //  an array of strings. Arguments to pass to the executable         &quot;env&quot;: null, //  An object with the environment to append to the current environment to execute the executable         &quot;revealOutputChannelOn&quot;: null, // A string. Specifies the condition when the output channel should be revealed         &quot;useRustfmt&quot;: null // either a boolean or null. Specifies whether rustfmt should be used for formatting    }}Finally it looks like rls.toml is being deprecated in favour ofsimiliary named settings.json parameters.sobuild_lib: falseworkspace_mode: truebecomes{    &quot;rust.build_lib&quot;: false,    &quot;rust.workspace_mode&quot;: true,}"
    },


    { 
        "title" : "London Rust User Group Meetup No. 14",

        
        
        
        "tags": [
        
            "rust",
            
        
            "meetup",
            
        
            "usergroup"
            
        
        ],
        "href" : "2017/london-rust-14",
        "content" : "InterwebsCambridge Meetup No. 2Request for Explanation Episode No. 2 - Stealing Chickens on the Internet. No. 4 landed on Monday!RustFest Zurich CFP: closes July 23rd 10PM UTCRequestsTalk Spots (add comment on the related issue if you want to talk)August 16th - 2 lightning talks (5-15m) availableSeptember 20th - 2 lightning talks (5-15m) availableMore dates - We're pre-booked with Skillsmatter up to the end of the year!JobsSee me after the talks if you want to advertise!"
    },


    { 
        "title" : "Cargo cult - the problem with copying existing code",

        
        
        
        "tags": [
        
            "cargocult",
            
        
            "visualstudio",
            
        
            "msbuild"
            
        
        ],
        "href" : "2017/cargo-cult-visual-studio",
        "content" : "A quick note to myself on how to identify build errors. This is also a timelyreminder that you should create code from scratch once in a while, rather thancopying an existing project. This is a form of cargo cult (I've linked to theWikipedia page in my references below, if you've never heard of the termbefore)The pain to get it working will be worthwhile, as you will have learntsomething new. In my case how to run MSBuild (Microsoft build toolchain) andidentify what version of SSIS (Microsoft flavoured ETL) my project was basedon.I started to wire up my brand new SSIS project to TeamCity (JetBrains flavouredCI). Immediately the build failed. Asking the team and no one else had seen theerror before.Googling suggested that I try and run MSBuild on TeamCity. One problem, Devsdon't get direct access to the CI/CD infrastructure. So time to learn how torun MSBuild locally.In the start menu &gt; All Programs &gt; VisualStudio 2015 &gt; VisualStudio Toolsthere's a handy shortcut called MSBuild Command Prompt for VS2015 this willsetup a command prompt with the correct config to run MSBuild.After copying over any dependencies for the build task (in my case the SSISDLLs from the TeamCity server), it was just a case of pointing MSBuild to mybuild file.C:\\Path\\To\\SSIS\\Package\\LOL&gt;msbuild LOL.buildMicrosoft (R) Build Engine version 14.0.25420.1Copyright (C) Microsoft Corporation. All rights reserved.Build started 19/07/2017 11:29:31.Project &quot;C:\\Path\\To\\SSIS\\Package\\LOL\\LOL.build&quot; on node 1 (default targets).SSISBuild:  **************Building SSIS project: LOL.dtproj **************  ------  Loading project file &#39;LOL.dtproj&#39;*snipping verbosity*  C:\\Path\\To\\SSIS\\Package\\LOL\\LOL.build(9,3): error : Error while loading   package &#39;WAT.dtsx&#39;: The package failed to load due to error 0xC0011008 &quot;Error   loading from XML. This occurs when CPackage::LoadFromXML fails.\\r*snipping verbosity*C:\\Path\\To\\SSIS\\Package\\LOL\\LOL.build(9,3): error :    at Microsoft.SqlServer        .Dts.Runtime.Package.LoadFromXML(String packageXml, IDTSEvents events)\\r    0 Warning(s)    2 Error(s) &lt;-- not entirely sure why MSBuild felt the need to run twice,                    but it did...Time Elapsed 00:00:01.16This was great, because this was exactly the error I got in TeamCity! Turns outour existing SSIS projects target SQL Server 2012. The new project inVisualStudio 2015 target SQL Server 2016 by default. So the error is literallyMSBuild saying, &quot;The SSIS DLLs don't know what the hell this file!&quot;To downgrade the project, it was just the case of right clicking on the SSISproject &gt; Configuration Properties &gt; General &gt; TargetServerVersion andswitching to SQL Server 2012.Then as if by magic...C:\\Path\\To\\SSIS\\Package\\LOL&gt;msbuild LOL.buildMicrosoft (R) Build Engine version 14.0.25420.1Copyright (C) Microsoft Corporation. All rights reserved.Build started 19/07/2017 11:10:42.Project &quot;C:\\Users\\staama1\\Source\\Workspaces\\ESPP\\Tasks\\SSIS\\LOL\\Main-Dev\\LOL\\LOL.build&quot; on node 1 (default targets).SSISBuild:  **************Building SSIS project: LOL.dtproj **************  ------  Loading project file &#39;LOL.dtproj&#39;  Setting output directory to &#39;bin\\Development&#39;  Setting project ProtectionLevel to &#39;DontSaveSensitive&#39;  Loading ConnectionManager &#39;ADONET.Chronos.conmgr&#39;  Loading ConnectionManager &#39;OLEDB.Chronos.conmgr&#39;  Loading package &#39;WAT.dtsx&#39;  Changing protection level of package to &#39;DontSaveSensitive&#39;  Saving project to: &#39;bin\\Development\\LOL.ispac&#39;Done Building Project &quot;C:\\Users\\staama1\\Source\\Workspaces\\ESPP\\Tasks\\SSIS\\    OPF_REPSYS\\Main-Dev\\LOL\\LOL.build&quot; (default targets).Build succeeded.    0 Warning(s)    0 Error(s)Time Elapsed 00:00:01.43ReferencesCargo cultMSBuild walkthroughBuilding MSBuild Project From Scratch"
    },


    { 
        "title" : "SSIS Variable Dispenser Template",

        
        
        
        "tags": [
        
            "ssis",
            
        
            "bids",
            
        
            "template"
            
        
        ],
        "href" : "2017/variable-dispenser",
        "content" : "I don't write enough SSIS script blob tasks to commit this to memory. This is the safest way to access variables without inadvertantly locking them after a crash.public void Main(){    Variables vars = null;    try    {        Dts.VariableDispenser.LockForWrite(&quot;User::strWritable&quot;);        Dts.VariableDispenser.LockForRead(&quot;User::strReadable&quot;);        Dts.VariableDispenser.GetVariables(ref vars);        vars[&quot;User::strWritable&quot;].Value = vars[&quot;User::strReadable&quot;].Value.ToString();        Dts.TaskResult = (int)ScriptResults.Success;    }    catch (Exception)    {        Dts.TaskResult = (int)ScriptResults.Failure;    }    finally    {        vars.Unlock();    }}"
    },


    { 
        "title" : "ORDS tips",

        
        
        
        "tags": [
        
            "ords",
            
        
            "xml",
            
        
            "mime_types",
            
        
            "oracle"
            
        
        ],
        "href" : "2017/ords",
        "content" : "XML over ORDSTurns out you can emit XML via ORDS. I'll assume you have a working ORDS install and the schema is already enabledBEGIN  ORDS.DEFINE_MODULE(      p_module_name    =&gt; &#39;reporting&#39;,      p_base_path      =&gt; &#39;/reporting/&#39;,      p_items_per_page =&gt;  0,      p_status         =&gt; &#39;PUBLISHED&#39;,      p_comments       =&gt; NULL);        ORDS.DEFINE_TEMPLATE(      p_module_name    =&gt; &#39;reporting&#39;,      p_pattern        =&gt; &#39;hello_xml&#39;,      p_priority       =&gt; 0,      p_etag_type      =&gt; &#39;HASH&#39;,      p_etag_query     =&gt; NULL,      p_comments       =&gt; NULL);  ORDS.DEFINE_HANDLER(      p_module_name    =&gt; &#39;reporting&#39;,      p_pattern        =&gt; &#39;hello_xml&#39;,      p_method         =&gt; &#39;GET&#39;,      p_source_type    =&gt; &#39;plsql/block&#39;,      p_items_per_page =&gt;  0,      p_mimes_allowed  =&gt; &#39;&#39;,      p_comments       =&gt; NULL,      p_source         =&gt; &#39;DECLARE    l_clob   CLOB;BEGIN    SELECT        &#39;&#39;&lt;xml&gt;hello world&lt;/xml&gt;&#39;&#39;    INTO        l_clob    FROM        dual;    owa_util.mime_header(        ccontent_type   =&gt; &#39;&#39;text/xml&#39;&#39;,        bclose_header   =&gt; true,        ccharset        =&gt; &#39;&#39;ISO-8859-4&#39;&#39;    );    htp.print(l_clob);END;&#39;      );  COMMIT; END;$ curl -v http://oracle.rocks:8080/ords/scott/reporting/mime_works* About to connect() to oracle.rocks port 8080 (#0)*   Trying 1.2.3.4... connected* Connected to oracle.rocks (1.2.3.4) port 8080 (#0)&gt; GET /ords/scott/reporting/mime_works HTTP/1.1&gt; User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.27.1 zlib/1.2.3 libidn/1.18 libssh2/1.4.2&gt; Host: oracle.rocks:8080&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Content-Type: text/xml; charset=ISO-8859-4&lt; ETag: &quot;WaloI8WDL3PY2G0ZN6+I8C+c0FxVaUBuDc/v7LKXTpE6dTuJR1s2bLF/0hqW2fVzaXNYpr9TFXqucyoq6dO2Xw==&quot;&lt; Transfer-Encoding: chunked&lt;&lt;xml&gt;hello world&lt;/xml&gt;* Connection #0 to host oracle.rocks left intact* Closing connection #0Important item of note is the mime header, it's important that bclose_header is always true because this is literally the last HTTP header before the response body. Setting this to false will result in no data.ReferencesORACLE-BASEEmitting JSON via refcursorEarlier this year Tim Hall did an excellent talk called, &quot;Make the RDBMS Relevant Again with RESTful Web Services and JSON&quot; (youtube) at Oracle Code 2017. The big takeaway for me was to not throw away your existing investment and rewrite everything to fit the new technology (ORDS). Instead you should use ORDS to wrap around the existing objects.At the day job we use strongly typed reference cursor as an out parameter (this helps interoperability with Microsoft's SSRS). Here's the example &quot;reporting&quot; package.CREATE OR REPLACE PACKAGE foo_pkg AS     /* strongly typed reference cursors or gtfo */    TYPE bar_reftype IS RECORD (        object_name user_objects.object_name%TYPE,        object_type user_objects.object_type%TYPE    );    TYPE bar_refcur IS REF CURSOR RETURN bar_reftype;        PROCEDURE bar_get (p_recordset IN OUT bar_refcur);END foo_pkg;/CREATE OR REPLACE PACKAGE BODY foo_pkg AS    PROCEDURE bar_get ( p_recordset IN OUT bar_refcur )        AS    BEGIN        OPEN p_recordset FOR            SELECT                uo.object_name,                uo.object_type            FROM                user_objects uo            WHERE                ROWNUM &lt; 5;    EXCEPTION        WHEN OTHERS THEN            IF                p_recordset%isopen            THEN                CLOSE p_recordset;            END IF;            RAISE;    END bar_get;END foo_pkg;/Here's the ORDS wrapper, yes unfortunately you're going to need APEX_JSON, but the example should be easy enough to understand. It would be great if one day we didn't need this boilerplate, but for now this is a handy way of reusing code.BEGIN  ORDS.DEFINE_MODULE(      p_module_name    =&gt; &#39;foo&#39;,      p_base_path      =&gt; &#39;/foo/&#39;,      p_items_per_page =&gt;  0,      p_status         =&gt; &#39;PUBLISHED&#39;,      p_comments       =&gt; NULL);        ORDS.DEFINE_TEMPLATE(      p_module_name    =&gt; &#39;foo&#39;,      p_pattern        =&gt; &#39;bar/&#39;,      p_priority       =&gt; 0,      p_etag_type      =&gt; &#39;HASH&#39;,      p_etag_query     =&gt; NULL,      p_comments       =&gt; NULL);  ORDS.DEFINE_HANDLER(      p_module_name    =&gt; &#39;foo&#39;,      p_pattern        =&gt; &#39;bar/&#39;,      p_method         =&gt; &#39;GET&#39;,      p_source_type    =&gt; &#39;plsql/block&#39;,      p_items_per_page =&gt;  0,      p_mimes_allowed  =&gt; &#39;&#39;,      p_comments       =&gt; NULL,      p_source         =&gt; &#39;DECLAREl_cursor SYS_REFCURSOR;BEGIN        foo_pkg.bar_get(l_cursor);    apex_json.open_object;    apex_json.write(&#39;&#39;bar&#39;&#39;, l_cursor);    apex_json.close_object;END;&#39;      );  COMMIT; END;/"
    },


    { 
        "title" : "Add support for tags to Cobalt",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "tags",
            
        
            "taxonomies",
            
        
            "categories"
            
        
        ],
        "href" : "2017/cobalt-tags",
        "content" : "A while back I started adding a new front matter attribute to all my postscalled tags. I initially did this to improve my site search indexing (you canread about that here).After spending some time thinking about how to add tags to the site, and Ithink I've come up with a reasonable solution.The biggest problem with tags is that you can't easily codegen the templatelike the one I used for the documents collectionfeed to seed the lunrindexer.The code for the tag template looks like this.&lt;/ul&gt;{% endfor %}{% endfor %}    {% endif %}        {% break %}&lt;/a&gt;&lt;/li&gt;            {{ post.title }}&quot;&gt;{{post.permalink }}            &lt;li&gt;&lt;a href=&quot;/{% if tag == filter_tag %}        {% for tag in tags %}    {% assign tags = post.data.tags | replace: &quot; &quot;, &quot;&quot; |  split: &quot;,&quot; %}    {% for post in collections.posts.pages reversed %}&lt;/h1&gt;&lt;ul&gt;{{ filter_tag }}&lt;h1&gt;Posts tagged as The code is fairly trivially, grab the tags attribute turn it into an arrayof tags and then loop over it and add a link if the tag is found.If you were to create a template per tag, there's a risk of code duplication.After that, it's only a matter of time, when you &quot;fix&quot; one template, but forgetapply to the others.You could mitigate against this by recreating all the tag pages at build time.I may revisit this solution at a later date to see if there's any otherbenefits that might have been overlooked.In the end I decided to use the include liquid tag to reuse the tag templatecode, again I got the idea from Johann's excellent blogwhere he broke up his default layout into components using include. One thingto note about using the include tag is that you shouldn't include a frontmatter section otherwise this will also be rendered.tags/cobalt.liquidfront matterextends: default.liquidtitle: booyaa&#39;s boggy bloggypath: tags/cobalt/comment: collection of cobalt related posts---The only item of note here is the path, I've decided to utilise tags as asub-path. I've not created a index page for this sub-path, so it'll give a 404for now.content{% include &quot;_tag.liquid&quot; %}{% assign filter_tag = &quot;cobalt&quot; %}The tag page content literally define the tag filter that will be used by thetags template and then includes the tags template.The end result can be found here. I should add that, I'm stillthinking about a way to add this link within related posts, but I think that'sanother problem altogether! The source code can be foundhere."
    },


    { 
        "title" : "Setting up exercism python track with Visual Studio Code",

        
        
        
        "tags": [
        
            "python",
            
        
            "exercism",
            
        
            "pylint",
            
        
            "pytest",
            
        
            "pep8",
            
        
            "vscode"
            
        
        ],
        "href" : "2017/exercism-python",
        "content" : "Here's a fairly good setup for getting the python track of exercism workingwith virtual env and Visual Studio Code.virtual envcreate one, and install the following packages:pip install pylint autopep8 # alwayspip install pytest pytest-cache # minimumpip install pytest-pep8 pdb # bonusvisual studio codeDon Jayanmanne’s Python extensionunit testing (using virtualenv/pyvenv)Configure using Run All Unit Tests  which will addsettings.json{    &quot;python.unitTest.pyTestArgs&quot;: [        &quot;.&quot;    ],    &quot;python.unitTest.pyTestEnabled&quot;: true}ignore certain pylint warningssettings.json{    &quot;python.linting.pylintArgs&quot;: [        &quot;--disable=C0111&quot;     ]}setup command line toolsfirst time setupinstall command line tools so you can launch code using code .activate your virtual envnav to your exercism execise path (don’t open the main python folder, code can only handle a single exercise.code .verify virtual environment is being used trigger command pallet and run Python: Select Interpreter Workspace (this doesn’t work at the moment since downgrading to 3.5.2check that intellisense and linting are workingsetup unit testsuseful http://exercism.io/languages/python/testsQuestionshow do i get code to run unit tests on saveenable pdb on unit test failure"
    },


    { 
        "title" : "python tips",

        
        
        
        "tags": [
        
            "python",
            
        
            "homebrew",
            
        
            "raspbian"
            
        
        ],
        "href" : "2017/python",
        "content" : "A random collection of Python tips, I also write Python code in MacOS andraspbian so you'll see tips for those platforms.Docker images (python related)httpbin - docker run -it --rm -p 8000:8000 citizenstig/httpbinjupyter notebook - docker run -it --rm -p 8888:8888 jupyter/all-spark-notebookpylint messagesto suppress # pylint: disable=invalid-nameto see a list pylint --list-msgs | grep -i W05Returning something useful from a classstr methodclass foo:    def __str__(self):        return “something rendered nicely”SimpleHTTPServer for python 3python3 -m http.server 8080TabulatingPanda’s probably a better fit because it can generate all kinds of tables (markdown, ascii, html etc)If you need something quick use tabulate.&gt;&gt;&gt; list(zip(*table2))[(1, &#39;A&#39;, &#39;@&#39;), (2, &#39;B&#39;, &#39;$&#39;), (3, &#39;C&#39;, &#39;!&#39;)]&gt;&gt;&gt; print(tabulate([list(x) for x in zip(*table2)], headers=[&quot;i1&quot;,&quot;i2&quot;,&quot;i3&quot;]))  i1  i2    i3----  ----  ----   1  A     @   2  B     $   3  C     !&gt;&gt;&gt; table2[[1, 2, 3], [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;], [&#39;@&#39;, &#39;$&#39;, &#39;!&#39;]]versioningdowngrading to python 3.5.2 in homebrewbrew install https://raw.githubusercontent.com/Homebrew/homebrew-core/f377ed1a1b568d5624164146ec7d6855fe175826/Formula/python3.rbpython3 -Vbrew pin python3switching back to 3.6.0brew unpin python3brew switch python3 3.6.0updating python3 pypieasy_install3 -U pippython -m pip install —upgrade pip # betterupgrading pip (python2)pip install —upgrade pippip install —upgrade setuptoolsif it fails (pip’s a module, avoid using this unless you’re using debian)apt-get remove pipeasy_install pippip install —upgrade pip"
    },


    { 
        "title" : "youtube-dl gems",

        
        
        
        "tags": [
        
            "youtube-dl"
            
        
        ],
        "href" : "2017/youtube-dl",
        "content" : "A handy collection of youtube-dl incantations, remember it's not just foryoutube-dl! There's plenty more, but these are the one use I use on a dailybasis.If you're a windows users, these should work on your platform provided you change any directory references from / to \\. And remove \\ and concatenate the entire incantation into a single line.youtube-dl -F VIDEO_URL # see all video formats available for a given urlyoutube-dl -f mp4 VIDEO_URL # download specific formatyoutube-dl -f mp4 -o &#39;%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&#39; \\     &#39;YOUTUBE_PLAYLIST_URL&#39; # downloads playlist in number orderyoutube-dl --playlist-items ITEM_SPEC &#39;YOUTUBE_PLAYLIST_URL&#39; \\    # download items from a playlist where ITEM_SPEC is 1,2,3 or 1-3,4,9-100 youtube-dl --verbose --username PS_USER --password PS_PASS \\    --sleep-interval 200 -o &quot;%(playlist)s/%(chapter_number)s.%(chapter)s/%(playlist_index)s.%(title)s.%(ext)s&quot; \\    --restrict-filenames PLURALSIGHT_URL \\    # where PS_USER and PS_PASS are your pluralsight credentials"
    },


    { 
        "title" : "Adding an archive page to your Cobalt blog",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "liquid"
            
        
        ],
        "href" : "2017/adding-an-archive",
        "content" : "To avoid slowing down the index page, there's a point where you need to limithow many blog posts you want to appear on screen. This in turn presents anotherproblem, how do you then provide a way to display older posts? Enter an archivepage!archive.liquidfront matterextends: default.liquidtitle: archivepath: /archiveroute: archive---templateThis is a quick work around until I&#39;ve had a good think about a way to organise the archive.&lt;h2&gt;2017&lt;/h2&gt;&lt;ul&gt;{% endfor %{% endif %}&lt;/li&gt;{{ post.content | strip_html | truncatewords: 25, &#39;...&#39; }}&lt;/a&gt; - {{ post.title }}&quot;&gt;{{ post.permalink }}  &lt;li&gt;&lt;a href=&quot;/{% if year == &quot;2017&quot; %}{% assign year = post.permalink | truncate: 4, &quot;&quot; %}{% for post in collections.posts.pages %}&lt;/ul&gt;The template is placed in the root of your cobalt source dirctory. Yes it's abit of hack, but I'm still playing around with the format. I think eventually Imay even turn the archiving into a code generated process during build time."
    },


    { 
        "title" : "Flashback, what did this data look like previously?",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "flashback",
            
        
            "lsd"
            
        
        ],
        "href" : "2017/flashback",
        "content" : "I'm only scratching the surface of what you can do with flashbacks in Oracle.Our DBAs are absolute ninjas when it comes to using this witchcraft from Oracle.The example below lets us look at a data dictionary for materialized refreshgroups and what the values were an hour ago.CLEAR SCREENCOL name FORMAT a30COL rname FORMAT a30SELECT    name,    rnameFROM    all_refresh_children AS OF TIMESTAMP ( trunc(SYSDATE - 1 / 24) )WHERE    rname = &#39;RFG_GROUP2&#39;;where:1 / 24 is an hour ago36 / 24 is yesterday an hour and half ago"
    },


    { 
        "title" : "SQL Developer's new format hints",

        
        
        
        "tags": [
        
            "sqldeveloper",
            
        
            "csv",
            
        
            "format"
            
        
        ],
        "href" : "2017/sql-developer-format-hints",
        "content" : "One of my favourite cool features in SQL Developer is the ability to turn a sqlquery output into an entirely different format. It doesn't even requirebreaking out the import/export wizard.SPOOL C:\\TEMP\\foobar.csvSELECT /*csv*/ *  FROM foo  LEFT  JOIN bar    ON foo.id = bar.id WHERE foo.name LIKE &#39;%HURR%&#39;;  SPOOL OFFComplete list of formatsSELECT /*csv*/ * FROM scott.emp;SELECT /*xml*/ * FROM scott.emp;SELECT /*html*/ * FROM scott.emp;SELECT /*delimited*/ * FROM scott.emp;SELECT /*insert*/ * FROM scott.emp;SELECT /*loader*/ * FROM scott.emp;SELECT /*fixed*/ * FROM scott.emp;SELECT /*text*/ * FROM scott.emp;Full details and source can be  here."
    },


    { 
        "title" : "Bitmasks in SQL",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "sql",
            
        
            "bitmasks"
            
        
        ],
        "href" : "2017/bitmasks-sql",
        "content" : "bitmasks are really handy way to express predicates without becoming overlyverbose with parens and logical operators (AND and OR). Assume we have thefollowing table.| what        | wanted ||-------------|--------|| need me too | 4      || alpha       | 1      || beta        | 2      |with data as(select &#39;appears_in_both&#39;, 4 as wanted from dualunion allselect &#39;alpha&#39;, 1 as wanted from dualunion allselect &#39;beta&#39;, 2 as wanted from dual)...Get &quot;need me too&quot; and &quot;alpha&quot; together...select * from datawhere bitand(wanted, 5) &lt;&gt; 0; -- (4 + 1)Get &quot;need me too&quot; and &quot;beta&quot; together...select * from datawhere bitand(wanted, 6) &lt;&gt; 0; -- (4 + 2)"
    },


    { 
        "title" : "Dirty Dynamic SQL",

        
        
        
        "tags": [
        
            "dynamic",
            
        
            "sql",
            
        
            "danger",
            
        
            "oneliners",
            
        
            "codegen"
            
        
        ],
        "href" : "2017/dirty-dynamic-sql",
        "content" : "Not all dynamic sql is strictly for immediate execution, nor is it dirty (Ineeded aninteresting title). I learnt this tricks from my friend at work, he'sa big believer of using sql to code generate more sql.Altering column lengthYou'll still get an error when running the generated sql if you try to shrink a column.SELECT &#39;ALTER TABLE &#39; || table_name || &#39; MODIFY &#39; || column_name || &#39; VARCHAR2(&#39; || CASE column_name  WHEN &#39;FOO&#39; THEN &#39;10&#39;  ELSE &#39;20&#39; END || &#39;);&#39; AS sql  FROM user_tab_columns  WHERE column_name IN ( &#39;FOO&#39;,&#39;BAR&#39;)   AND table_name IN (SELECT table_name FROM user_tables)      AND NOT regexp_like(table_name, &#39;_(NEW|BAK)$&#39;) -- exclude backupsInserting (hurr) output from dynamic sqlNormally you'd expect to get away with using something like SELECT blah INTO v_foo, but with dynamic sql you need place the INTO in the EXECUTE IMMEDIATE statement.SET SERVEROUTPUT ONDECLARE  v_sql VARCHAR2(8000);  v_count NUMBER;  CURSOR c_tables      IS  SELECT table_name    FROM user_tables;BEGIN  FOR rec in c_tables  LOOP    v_sql := &#39;SELECT count(*) FROM &#39; || rec.table_name;    EXECUTE IMMEDIATE v_sql INTO v_count;        DBMS_OUTPUT.put_line(rec.table_name || &#39; has &#39; || v_count);  END LOOP;END;/tags: rowcount , row , countUsing substution variablesclear screenset serveroutput ondeclare  cursor c_tables is select table_name from user_tables;  v_data VARCHAR2(2000);  v_sql VARCHAR2(2000);  v_stuff VARCHAR2(50) := &#39;&amp;1&#39;; -- try entering MAX(DATADATE) or COUNT(*) begin  for rec in c_tables  loop    v_sql := &#39;SELECT TO_CHAR(&#39; || v_stuff || &#39;) FROM &#39; || rec.table_name;        begin      execute immediate v_sql into v_data;      dbms_output.put_line(rec.table_name || &#39;: &#39; || v_data);        exception      when others then        dbms_output.put_line(&#39;failed to run: &#39; || v_sql);        raise;    end;    end case;  end loop;end;/One linersGrantsSELECT &#39;GRANT &#39; || privilege || &#39; ON &quot;&#39; || GRANTOR ||&#39;&quot;.&quot;&#39; || TABLE_NAME ||        &#39;&quot; TO &quot;&#39; || GRANTEE || &#39;&quot;;&#39; FROM dba_tab_privs WHERE grantor = &#39;FOO&#39;AND grantee = &#39;BAR&#39;;Recreating synonyms as sysselect &#39;CREATE OR REPLACE SYNONYM &quot;&#39; || OWNER || &#39;&quot;.&quot;&#39; || SYNONYM_NAME ||        &#39;&quot; FOR &quot;&#39; || TABLE_OWNER || &#39;&quot;.&quot;&#39; || TABLE_NAME || &#39;&quot;;&#39; from dba_synonyms where table_owner = &#39;FOO&#39;;"
    },


    { 
        "title" : "When XML attacks!",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "xmltype",
            
        
            "xml",
            
        
            "extractvalue"
            
        
        ],
        "href" : "2017/when-xml-attacks",
        "content" : "At some point in your xml wrangling career you will hit an node whose data istoo big for Oracle's EXTRACTVALUE (I think the upper limit is 4000characters) and get this lovely message.ORA-01706: user function result value was too large01706. 00000 -  &quot;user function result value was too large&quot;To mitigate this problem, you need to switch to XMLTABLE and it's usefulPASSING attribute:SELECT    fix.biggie AS notoriousFROM    source_table_with_xmltype_column o,    XMLTABLE ( &#39;/*&#39;        PASSING o.xml_field COLUMNS            biggie VARCHAR2(4000) PATH &#39;substring(/path/to/offending/item/text(),1,3999)&#39;    ) fix;The gist of the fix is to use xpath to truncate the text value before it'shanded off to Oracle. Trying to cast the xmltype column as a varchar2(4000)will not fix it because the problem happens as extractvalue is parsing thenode.Items of noteXML ( '/*' - how to specify the root of an xml documentbiggie VARCHAR2(4000) PATH 'substring(/path/to/offending/item/text(),1,3999)'how to get the text from an xpath and truncate it to 4000 chars.sources:XMLTABLE : Convert XML Data into Rows and Columns using SQLSO (Answer): How to use xmltable in oracle?SO (Answer): Oracle SQL - Extracting clob value from XML with repeating nodes"
    },


    { 
        "title" : "Bind vs Substitution variables",

        
        
        
        "tags": [
        
            "oracle",
            
        
            "plsql",
            
        
            "variables",
            
        
            "bind",
            
        
            "substitution"
            
        
        ],
        "href" : "2017/bind-vs-substitution",
        "content" : "I always have difficulty remember the difference between these type ofvariables. Although now, that I've started doing a lot of ORDS related work,the difference is become more apparent.Also substitution variables are really a binding to a user variable.Bind variablesPROMPT bind variables are...VAR foo VARCHAR2SET SERVEROUTPUT ONBEGIN  :FOO := &#39;in PL/SQL blocks&#39;;  DBMS_OUTPUT.PUT_LINE(&#39;mostly used...&#39;);END;/SET SERVEROUTPUT OFFPRINT foooutputbind variables are...PL/SQL procedure successfully completed.mostly used...FOO---in PL/SQL blocksSubstitution variablesPROMPT Where as substitution variable are...DEFINE FOO = &#39;useful in SQL scripts&#39;SET VERIFY OFFSELECT &#39;&amp;FOO&#39; AS BAR FROM DUAL;SET VERIFY ONoutputWhere as user variable are...BAR                 ---------------------useful in SQL scriptsTo be explict, use bind vars to interact with PL/SQL blocks and substituionvariables could be used anywhere. The only downside to substituion variables isthat you can't DEFINE a variable with another user variable.Also you can break substitution variables, if there's an errant SET DEFINE OFF,still not sure how you could test for this? Perhaps using default valuesand testing for it i.e. like SQLCMD variables.Further reading:Literals, Substitution Variables and Bind VariablesPL/SQL 101 : Substitution vs. Bind Variables"
    },


    { 
        "title" : "PL/SQL script to query a refcursor",

        
        
        
        "tags": [
        
            "cursors",
            
        
            "oracle",
            
        
            "plsql"
            
        
        ],
        "href" : "2017/plsql-script-cursors",
        "content" : "This will probably work in pipelined functions or packages too. Note the use ofthe bind variable to link the PL/SQL script variables to the out refcursor.SET SERVEROUTPUT ONCLEAR SCREENVAR P_NAME CHARVAR P_TABLES REFCURSORDECLARE  PROCEDURE table_get(p_name IN VARCHAR2, p_tables OUT SYS_REFCURSOR)  AS  BEGIN    OPEN p_tables      FOR        SELECT *           FROM user_tables          WHERE TABLE_NAME LIKE P_NAME;  END;BEGIN    :P_NAME := &#39;%REQ%&#39;;  table_get(:P_NAME, :P_TABLES);END;/PRINT P_TABLES"
    },


    { 
        "title" : "Defensive coding in SQL",

        
        
        
        "tags": [
        
            ""
            
        
        ],
        "href" : "2017/defensive-coding-sql",
        "content" : "Always wrap ON clauses in parens to avoid predicates being deletedaccidentally. The following code will scream if you delete the AND clause.SELECT *  FROM foo  LEFT JOIN bar    ON ((foo.id = bar.id)        AND (foo.fizz = bar.buzz))Where as the following won't.SELECT *  FROM foo  LEFT JOIN bar    ON foo.id = bar.id        AND foo.fizz = bar.buzz"
    },


    { 
        "title" : "Collapsible Sections in HTML",

        
        
        
        "tags": [
        
            "html",
            
        
            "github"
            
        
        ],
        "href" : "2017/collapsible-sections",
        "content" : "But first a demo...Works great in GitHub! (click me)  Dipshit with a nine-toed woman. Dolor sit amet, consectetur adipiscing elit praesent ac magna. You don’t go out and make a living dressed like that in the middle of a weekday. Justo pellentesque ac lectus quis. Yeah. Roadie for Metallica. Speed of Sound Tour. Elit blandit fringilla a ut turpis praesent felis. Keep your ugly fucking goldbricking ass out of my beach community! Ligula, malesuada suscipit malesuada non, ultrices non urna sed orci ipsum, placerat id. Code&lt;details&gt;  &lt;summary&gt;Click to expand...&lt;/summary&gt;  &lt;p&gt;Imagine this is the entire text of War and Peace.&lt;/p&gt;&lt;/details&gt; "
    },


    { 
        "title" : "See hidden files in Finder",

        
        
        
        "tags": [
        
            "mac",
            
        
            "finder"
            
        
        ],
        "href" : "2017/hidden-files",
        "content" : "If you search for how to do this, you get a lot of nonsense involving messing with defaults write and horrific applescript bodges.CMD + SHIFT + .That's command key, shift and full stop (period) pressed at the same time."
    },


    { 
        "title" : "Using pre-built lunr indexes",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "search",
            
        
            "lunr"
            
        
        ],
        "href" : "2017/prebuilt-lunr-indexes",
        "content" : "I've implemented pre-built indexes vs. on demand i.e. generating them when thesearch page is being loaded. I'm not entirely happy with the solution yet forthe following reasons:Additional requirements - pre-built indexes require node.js to generate theindexes offline.Documents collection - the original search page still depends on the reffield (the link to the relevant blog post) and title because the lunr indexdoesn't provide this. The content field is now surplus to requirements oncethe site has been built, but it is needed during index generation.Added complexity - I know adding node.js isn't a big ask, but it has resultedin a new makefile. And until I update my travis config, pre-built indexes onlyget created when I've written a new article.One novel approach to solving both these problems is to utilise a V8 crate andrun lunr index natively. This is the approach adopted by the lunr indexerfor middleman (a static site generate written inruby).Since we're not looking at native Cobalt solutions i.e. using liquid templates,another approach would be to modify the node.js script used to pre-build theindex, to parse the content natively and remove the need for the lunr.liquidtemplate. The script would also need to generate the documents collectionthat's still used by the search page minus the content field.The code changes can be found in the following commits:node.js script - used to pre-build the index, mostly cribbed from the lunrguide on that subject.makefile and search template - the makefile is cribbed from Matthias Endler's own version used tobuild his Cobalt site. The search template adds the prebuilt index to the list ofassets to be loaded when the page is loaded."
    },


    { 
        "title" : "Useful git commmands",

        
        
        
        "tags": [
        
            "git"
            
        
        ],
        "href" : "2017/useful-git-commands",
        "content" : "Commit logsNow that I've started adding useful commit messagesit's really use to make sense of my commits, more so when you use single linemode.$ git log --oneline3b04513 doc: add &#39;comment&#39; tag to explain purpose of new templates86cc454 feat: add tags to search7286f90 feat: add twitter share button24cfc85 feat: add todo20d28d7 chore: remove manually generated lunr index...Started working on a feature/fix, but forgot you we're in master?This assumes you haven't committed your changes...git checkout -b new_branch# add and commit your changesgit checkout master# clean up any files you don&#39;t want to commit...Sync your repo with the upstreamSetup a remote to the upstream.git remote add upstream &lt;path/to/upstream&gt; # e.g. https://github.com/cobalt-org/liquid-rust.gitNow sync!git fetch upstream  # gets branches and commits from upstreamgit checkout master # switch to your fork’s master branch (if you&#39;re not already there)git merge upstream/master # syncgit push -u origin master # update remote repo of your forkThis can also be used to push your GitHub repos to other hosted SCM providersi.e. GitLab and BitBucketSubmodulesYou want to add repo Bar to your own repo Foogit submodule add https://path/to/barTo pull the changes ingit submodule initTo do this at clone timegit clone --recursive https://repo/contain/submodulesTo refresh the submodule (assuming the dir name is the same as the submodule)git submodule update --remote submodule_namePossible gotcha if you refresh your submodule, you will need to commit newversion into your own repo.TagsN.B. Tags when push to GitHub become releases!to taggit tag v1.0.0to see what tag we're ongit tagto pushgit push --tagsto pullgit fetch --tagsFixing &quot;fatal: refusing to merge unrelated histories&quot;Warning: this is destructive, if you've got local uncommitted changes do notproceed!$ git pull origin masterFrom github.com:booyaa/broken_repo * branch            master     -&gt; FETCH_HEADfatal: refusing to merge unrelated histories$ git fetch # may be superflurous$ git reset --hard origin/master$ git branch --set-upstream-to=origin/master masterBranch master set up to track remote branch master from origin.Updating a branch with new changes from mastergit checkout out_of_date_branchgit merge origin/mastergit push origin out_of_date_branch"
    },


    { 
        "title" : "Adding search to your cobalt site - Part Two",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github",
            
        
            "search",
            
        
            "lunr",
            
        
            "liquid"
            
        
        ],
        "href" : "2017/adding-search-to-your-cobalt-site-part-two",
        "content" : "This will be a two part post, where I detail the steps it took to enablesearch on my Cobalt site.As you may have gathered in part one,creating manual document collections is a bit of a chore, and can be easily doneusing the liquid templating engine.lunr.liquidfront mattertitle: lunr indexpath: /js/lunr_docs.json---The item of note here, is the path which Cobalt will use to create the lunrdocument collection.content]{% endfor %}    }{% if idx &lt; post_count %},{% endif %}        &quot;content&quot; : &quot;{{ post.content | strip_html | strip_newlines                                      | replace: &quot;\\&quot;, &quot;\\\\&quot; }}&quot;        &quot;href&quot; : &quot;{{ post.permalink }}&quot;,        &quot;title&quot; : &quot;{{ post.title }}&quot;,    { {% assign idx = idx | plus: 1 %}{% for post in collections.posts.pages %}[{% assign post_count = posts | size %}{% assign idx = 0 %}The only real difference here between a blog index is that I'm tracking the lastpost using an index so I can omit a trailing comma.Don't forget to remove the manually generated document collection(lunr_docs.json) from the assets directory for javascript files (js/) , asthis caught me out and made me wonder why the index wasn't being updated.Updates2017-06-21 - Change references to the lunr index to the lunr documentcollection."
    },


    { 
        "title" : "Adding search to your cobalt site - Part One",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github",
            
        
            "search",
            
        
            "lunr",
            
        
            "liquid"
            
        
        ],
        "href" : "2017/adding-search-to-your-cobalt-site-part-one",
        "content" : "This will be a two part post, where I detail the steps it took to enablesearch on my Cobalt site.In this first post I will detail how to integrate lunrusing a manually created document collection. If you already know how to wireup lunr, you can skip to second post,where I create the document collection using a liquid template.I love blogs, but after the initial excitement of discovery you become curiousabout the blog author and if they have other posts of a similar topic. At thispoint you expect to have either a local search or some form of taxonomy.This is often a shortcoming of using a static site generator to power yourblog. It's great at producing pre-rendered static pages, but search or tagviews are the domain of a dynamic content management system.As the title of this blog post suggests, I will be looking to solve the problemof local search. I'll leave the subject of taxonomies for a future blog post.It turns out that we can approximate a resonable search experience usinglunr. lunr is a light weight implementation ofSolr the enterprise search platform (off theshelf search engine). With a little bit of jQuery and the lunr library we cancobble together a search page in Cobalt.My Cobalt source directory structureI've shared my own blog's structure as an way to refer to locations within aCobalt source directory..├── _drafts├── _layouts├── blog├── img└── jsThere should be no surprises here, our primary areas of interest will be thejs directory where javascript related assets live. And the root of thedirectory where my index, about and licenses pages live (all extending thedefault liquid template).search.liquidI placed my search template in the root of my Cobalt source and it looks likethis.frontmatterextends: default.liquidtitle: searchpath:  search/---contentThe general gist (pun intended) of the template is to create an text input boxand a hook for the results to appear in.The template is a copy of this gistfor getting lunr to work with hugo (anexcellent static site generator written in Go).It has some tweaks to get it working with lunr v2.1.0.&lt;input id=&quot;search&quot; type=&quot;text&quot; size=&quot;25&quot; placeholder=&quot;search for stuff here...&quot;        autofocus&gt;&lt;br /&gt;&lt;ul id=&quot;results&quot;&gt;&lt;/ul&gt;&lt;script type=&quot;text/javascript&quot;         src=&quot;https://code.jquery.com/jquery-2.1.3.min.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;https://unpkg.com/lunr/lunr.js&quot;&gt;&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt;var lunrIndex,    $results,    pagesIndex;// This is pretty much a copy of // https://gist.github.com/sebz/efddfc8fdcb6b480f567// Initialize lunrjs using our generated index filefunction initLunr() {    // First retrieve the index file    $.getJSON(&quot;/js/lunr_docs.json&quot;)        .done(function(index) {            pagesIndex = index;            // Set up lunrjs by declaring the fields we use            // Also provide their boost level for the ranking            lunrIndex = lunr(function() {                this.field(&quot;title&quot;, {                    boost: 10                });                this.field(&quot;content&quot;);                // ref is the result item identifier (I chose the page URL)                this.ref(&quot;href&quot;);                                // Feed lunr with each file and let lunr actually index them                pagesIndex.forEach(function(page) {                    this.add(page)                }, this);            });                    })        .fail(function(jqxhr, textStatus, error) {            var err = textStatus + &quot;, &quot; + error;            console.error(&quot;Error getting cobalt index file:&quot;, err);        });}// Nothing crazy here, just hook up a listener on the input fieldfunction initUI() {    $results = $(&quot;#results&quot;);    $(&quot;#search&quot;).keyup(function() {        $results.empty();        // Only trigger a search when 2 chars. at least have been provided        var query = $(this).val();        if (query.length &lt; 2) {            return;        }        var results = search(query);        renderResults(results);    });}/**    * Trigger a search in lunr and transform the result    *    * @param  {String} query    * @return {Array}  results    */function search(query) {    // Find the item in our index corresponding to the lunr one to have more     // info    // Lunr result:     //  {ref: &quot;/section/page1&quot;, score: 0.2725657778206127}    // Our result:    //  {title:&quot;Page1&quot;, href:&quot;/section/page1&quot;, ...}    return lunrIndex.search(query).map(function(result) {            return pagesIndex.filter(function(page) {                return page.href === result.ref;            })[0];        });}/**    * Display the 10 first results    *    * @param  {Array} results to display    */function renderResults(results) {    if (!results.length) {        return;    }    // Only show the ten first results    results.slice(0, 10).forEach(function(result) {        var $result = $(&quot;&lt;li style=\\&quot;list-style:none;\\&quot;&gt;&quot;); // FUUUUUU!        $result.append($(&quot;&lt;a&gt;&quot;, {            href: result.href,            text: &quot;» &quot; + result.title        }));        $results.append($result);    });}// Let&#39;s get startedinitLunr();$(document).ready(function() {    initUI();});&lt;/script&gt;An artisanal lunr document collectionTo get my proof of concept going, I needed to feed lunr a distilled form of myblog posts, which I called lunr_docs.json and stored it in /js. In anearlier version of this post, I mistakenly referred to this as the lunr index.It's the data that will be used to generate lunr's index. The index hasdifferent structure that we'll learn about in part two.[{    &quot;title&quot;: &quot;Useful commit messages&quot;,    &quot;href&quot;: &quot;/2017/useful-commit-messages/&quot;,    &quot;content&quot;: &quot; Keeping a copy of this excellent bit of advice until I&#39;ve committed (no pun) it to memory. &quot;}, {    &quot;title&quot;: &quot;Add reading time in Cobalt&quot;,    &quot;href&quot;: &quot;/2017/add-reading-time/&quot;,    &quot;content&quot;: &quot; I wanted to add an approximate reading time to each of my blog posts, like those seen in medium posts. &quot;}, {    &quot;title&quot;: &quot;Using a custom domain with GitHub Pages&quot;,    &quot;href&quot;: &quot;/2017/gh-pages-custom-domain/&quot;,    &quot;content&quot;: &quot; It took far too long to work out how to do this on the GitHub help pages... &quot;}, {    &quot;title&quot;: &quot;Using Cobalt with GitHub pages&quot;,    &quot;href&quot;: &quot;/2017/cobalt-github/&quot;,    &quot;content&quot;: &quot; It turns out using Cobalt and your personal GitHub page is a bit trickier to setup. Your personal GitHub page as oppose to your repo GitHub page, must have the content in the master branch. Repository/Project GitHub pages can live in a subdir of default branch i.e. docs &quot;}, {    &quot;title&quot;: &quot;MacBook Air Setup&quot;,    &quot;href&quot;: &quot;/2017/mba-setup/&quot;,    &quot;content&quot;: &quot; Here&#39;s my current setup for my MacBook Air Setup. I use a range of tools like homebrew, Visual Studio Code and vim. &quot;}]The format of the document collection is fairly trivial and only has a verysmall fragment of the blog post, which will affect searching.Incidentally the boost property for the title field in the search templateis probably superflorous as it's the only item being searched again. Theoriginal source for the code also utilised a tag field, and boost allowsyou to give a weighting for which field should be favoured when searching theindex.As you can imagine hand crafting an document collection is a bit lo-fi, so ifyou want to see what I used in the end (another liquid template), check outpart two of this blog post.Putting it all togetherOnce you've created the search template and the lunr index, all you need to dois perform your usual cobalt build workflow.If you've followed my structure, your search page can be found in /search.Search results should appear immediately as you start to type in the input box.But is it webscale?I have no idea, I don't have a large enough volume of data to test against.However lunr v2.x's lunr.Index.load function allows you to load a pre-builtindex, but this does add an extra of complexity. And at time of writing willrequire either node.js or some form of v8 context to generate it. The idea isthat you index the document collection and serialise the index generated.More details can be found here.Anything else?I'd love to remove the dependency on jQuery, but to be fair I can't be arsedto rewrite in vanilla js as it just works ™. Saying that, it has got methinking perhaps, I should create a liquid-rust extension (not sure if it'll bea tag, filter or other) to generate vanilla js for things that causes people toreach for jQuery.Also I have no idea why the damn input box is so small.Any guidence or help with either of these two issues would be greatlyappreciated.Updates2017-06-21 - Change references to the lunr index to the lunr documentcollection."
    },


    { 
        "title" : "Useful commit messages",

        
        
        
        "tags": [
        
            "git"
            
        
        ],
        "href" : "2017/useful-commit-messages",
        "content" : "Keeping a copy of this excellent bit of advice until I've committed (no pun) itto memory.feat: add hat wobble^--^  ^------------^|     ||     +-&gt; Summary in present tense.|+-------&gt; Type: chore, docs, feat, fix, refactor, style, or test.Source: https://seesparkbox.com/foundry/semantic_commit_messages"
    },


    { 
        "title" : "Add reading time in Cobalt",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github",
            
        
            "liquid"
            
        
        ],
        "href" : "2017/add-reading-time",
        "content" : "I wanted to add an approximate reading time to each of my blog posts, like those seen in medium posts. There's a lot of really nice javascript libraries out that, but I kinda figured this defeat the whole purposeof using a static site generator.I was also looking for an excuse to use liquid (the template rendering engine for Cobalt).It's a fairly simplistic approach, perform a word count and divide by 200 which is the average of words read per min.{% assign reading_wpm = 200 %}{% assign word_count = page.content | split: &quot; &quot; | size %}{% assign reading_time = word_count | divided_by: 200 %}{% case reading_time %}  {% when 0 %}    {% assign phrase = &quot;less than a minute.&quot; %}  {% when 1 %}    {% assign phrase = &quot;about a minute.&quot; %}  {% else %}    {% assign phrase = &quot; minutes.&quot; | prepend: reading_time %}{% endcase %}Reading time: {{ phrase }}"
    },


    { 
        "title" : "Using a custom domain with GitHub Pages",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github",
            
        
            "dns"
            
        
        ],
        "href" : "2017/gh-pages-custom-domain",
        "content" : "It took far too long to work out how to do this on the GitHub help pages...Assumptions:I've only tested for personal/user domain i.e. the doc root forhttp://USERNAME.github.io/.You've already have an A (APEX) record for your existing site.You've already got a CNAME record that points to the A record.InstructionsEnable custom domain in your repository (settings).Update your A record to point to IP addresses: 192.30.252.153 and192.30.252.154. Pro-tip: Switch your DNS management to cloudflare if you wantsuper fast switch from your old hosting to GitHub.echo &quot;your-domain-name&quot; &gt; CNAME in the default branch repo. This will besource if you're doing this for a Cobalt site."
    },


    { 
        "title" : "Using Cobalt with GitHub pages",

        
        
        
        "tags": [
        
            "cobalt",
            
        
            "github"
            
        
        ],
        "href" : "2017/cobalt-github",
        "content" : "It turns out using Cobalt and yourpersonal GitHub page is a bit trickier to setup. Your personal GitHub page asoppose to your repo GitHub page, must have the content in the master branch.Repository/Project GitHub pages can live in a subdir of default branch i.e.docsThis is not a criticism of Cobalt, but rather myself demonstrating my lack ofgit prowess.The general gist of this how to, is that you place your Cobalt project in thesource branch and then use cobalt import --branch master to transfer therendered content to master.InstructionsCreate your user/personal page repo (USERNAME.github.io)Follow instructions for initialising a repogit checkout -b sourcecobalt init# do cobalty stuff..cobalt build# commit, maybe even push to sourcecobalt import --branch mastergit checkout master# commit, definitely pushIf you want to check out my setup you can find it here.Things I've not worked out yetThe build folder gets copied to the master branch during cobalt import.The folder doesn't contain any files. My work around at the moment is to add itto .gitignore in the master branch.Thanks to...Johann Hofmann for allowing me to copy hisblog's style (and his travis-ci auto deploy setup).Cobalt for creating this easy touse static site generator."
    },


    { 
        "title" : "MacBook Air Setup",

        
        
        
        "tags": [
        
            "mac",
            
        
            "homebrew",
            
        
            "vscode",
            
        
            "vim",
            
        
            "setup"
            
        
        ],
        "href" : "2017/mba-setup",
        "content" : "Here's my current setup for my MacBook Air Setup. I use a range of tools likehomebrew, Visual Studio Code and vim.homebrewbrew install \\        python python3 bash-git-prompt elixir figlet ffmpeg go httpie \\         imagemagick jq nmap zeromq reattach-to-user-namespace tmux sqlite \\         watch vim yarn youtube-dl doctlvscode extensionsgitlenshipsumInsert Date StringLiquid LanguagePrettyify JSONPython (Don's)vscode-rustvim plugins$ ls ~/.vim/bundle/go-explorer rust.vim  tagbar       vim-fugitive  vim-go    webapi-vimnerdtree    syntastic tcomment_vim vim-gitgutter vim-racer"
    },


    { 
        "title" : "Code generation scripts in PL/SQL",

        
        
        
        "tags": [
        
            "codegen",
            
        
            "plsql",
            
        
            "oracle",
            
        
            "xml"
            
        
        ],
        "href" : "2017/flattening-xml",
        "content" : "Flattening XML pathsThe table elephant_castle has a XMLTYPE column details that we want toget a list of distinct node paths.WITH node_list AS (    SELECT        x.*    FROM        elephant_castle ec        CROSS JOIN XMLTABLE ( &#39;declare function local:path-to-node( $nodes as node()* ) as xs:string* {     $nodes/string-join(ancestor-or-self::*/name(.),&#39;&#39;/&#39;&#39;) };for $i in $doc//*    let $node_path := local:path-to-node($i)      (: still don&#39;&#39;t have a clue how this func works :)    let $node_value := $i/text()    where string-length($node_value) &gt; 0            (: how to exclude nodes without values :)    return &lt;data&gt;                &lt;path&gt;{$node_path}&lt;/path&gt;                &lt;value&gt;{$node_value}&lt;/value&gt;            &lt;/data&gt;&#39;                PASSING ec.details AS &quot;doc&quot; COLUMNS                    node_path VARCHAR2(4000) PATH &#39;path&#39;,                    node_value VARCHAR2(4000) PATH &#39;value&#39; /* debugging */            ) x) SELECT distinct nl.node_pathFROM    node_list nl;    Generate a query that combines existing fields with newly flatten XML pathsThis script allows you to apply boiler plate (fields from the existing tableusing the data dictionary) and then make call out to another sqlplus scriptthat contains table specific mappings. The call out script is assumed to becalled TableName.sql.REM Suppress all headings, page breaks, titlesSET PAGESIZE 0REM Do not list the text of a command before and after replacing substitution REM variables with valuesSET VERIFY OFFREM  Do not display the number of records returned (when rows &gt;= n )SET FEEDBACK OFFCLEAR SCREENDEFINE TableName=STANDINGORDERDEFINE TableAlias=soDEFINE TableSchema=BPHADMIN/********************************************************************************* Generate existing column list, excluding XMLTYPE, BLOB and CLOB types.*******************************************************************************/WITH column_list AS (    SELECT        column_id,        column_name    FROM        all_tab_cols    WHERE        owner = &#39;&amp;TableSchema&#39;    AND        table_name = &#39;&amp;TableName&#39;    AND        data_type NOT IN (            &#39;XMLTYPE&#39;,&#39;BLOB&#39;,&#39;CLOB&#39;        )    ORDER BY column_id) SELECT    CASE column_id          WHEN 1 THEN &#39;SELECT &amp;TableAlias&#39;|| &#39;.&#39; || column_name        ELSE &#39;,&amp;TableAlias&#39;|| &#39;.&#39; || column_name    END as sqlFROM    column_list;/********************************************************************************* Call out script with custom XML flattening logic (usually field name tweaks)*******************************************************************************/@&amp;TableName/********************************************************************************* Add &quot;FROM&quot; statement*******************************************************************************/PROMPT FROM &amp;TableName &amp;TableAlias;;"
    }

]
